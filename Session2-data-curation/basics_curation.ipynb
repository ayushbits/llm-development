{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0936ea03-3f38-49e9-91c3-2d370769c8a8",
   "metadata": {},
   "source": [
    "# 1. Basics of Data Curation\n",
    "\n",
    "******\n",
    "\n",
    "Generative AI developemet requires a havy data curation process. The quality the model largely depends on the quality of the data used for training. NVIDIA NeMo Curator is an open-source framework designed to streamline this process by preparing large-scale, high-quality datasets for pretraining and continuous training.\n",
    "\n",
    "NeMo Curator offers built-in workflows for curating data from various public sources such as Common Crawl, Wikipedia, and arXiv. At the same time, it provides the flexibility to customize pipelines to suit the specific needs of your project.\n",
    "\n",
    "This notebook guides the process of basic data preparation involved in most Language Models developements: \n",
    "\n",
    "**[1.1 Text Cleaning and Unification](#1.1-Text-Cleaning-and-Unification)<br>**\n",
    "**[1.2 Document Size Filtering](#1.2-Document-Size-Filtering)<br>**\n",
    "**[1.3 Filter Personally Identifiable Information (PII)](#1.3-Filter-Personally-Identifiable-Information-(PII))<br>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfc4b2-8fd0-4cbf-86ae-094e6c8fead9",
   "metadata": {},
   "source": [
    "***************\n",
    "### Environment Setup\n",
    "\n",
    "For large-scale data processing, NeMo Curator provides both GPU and CPU based modules. Understanding how these modules interact and how to configure your environment is key to optimizing performance.\n",
    "\n",
    "CPU-based modules rely on [Dask](https://www.dask.org/) to distribute computations across multi-node clusters while GPU-accelerated modules uses [RAPIDS](https://rapids.ai/) to handle large-scale datasets efficiently.\n",
    "\n",
    "Let's check first your current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652486cf-06ef-43af-b136-96cda3b7714c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:             x86_64\n",
      "  CPU op-mode(s):         32-bit, 64-bit\n",
      "  Address sizes:          46 bits physical, 57 bits virtual\n",
      "  Byte Order:             Little Endian\n",
      "CPU(s):                   26\n",
      "  On-line CPU(s) list:    0-25\n",
      "Vendor ID:                GenuineIntel\n",
      "  Model name:             Intel(R) Xeon(R) Platinum 8480+\n",
      "    CPU family:           6\n",
      "    Model:                143\n",
      "    Thread(s) per core:   1\n",
      "    Core(s) per socket:   1\n",
      "    Socket(s):            26\n",
      "    Stepping:             8\n",
      "    BogoMIPS:             4000.00\n",
      "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n",
      "                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall\n",
      "                           nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_go\n",
      "                          od nopl xtopology cpuid tsc_known_freq pni pclmulqdq v\n",
      "                          mx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe\n",
      "                           popcnt tsc_deadline_timer aes xsave avx f16c rdrand h\n",
      "                          ypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd i\n",
      "                          brs ibpb stibp ibrs_enhanced tpr_shadow flexpriority e\n",
      "                          pt vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2\n",
      "                           erms invpcid avx512f avx512dq rdseed adx smap avx512i\n",
      "                          fma clflushopt clwb avx512cd sha_ni avx512bw avx512vl \n",
      "                          xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wb\n",
      "                          noinvd arat vnmi avx512vbmi umip pku ospke waitpkg avx\n",
      "                          512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bita\n",
      "                          lg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemot\n",
      "                          e movdiri movdir64b fsrm md_clear serialize tsxldtrk a\n",
      "                          vx512_fp16 arch_capabilities\n",
      "Virtualization features:  \n",
      "  Virtualization:         VT-x\n",
      "  Hypervisor vendor:      KVM\n",
      "  Virtualization type:    full\n",
      "Caches (sum of all):      \n",
      "  L1d:                    832 KiB (26 instances)\n",
      "  L1i:                    832 KiB (26 instances)\n",
      "  L2:                     104 MiB (26 instances)\n",
      "  L3:                     416 MiB (26 instances)\n",
      "NUMA:                     \n",
      "  NUMA node(s):           1\n",
      "  NUMA node0 CPU(s):      0-25\n",
      "Vulnerabilities:          \n",
      "  Gather data sampling:   Not affected\n",
      "  Itlb multihit:          Not affected\n",
      "  L1tf:                   Not affected\n",
      "  Mds:                    Not affected\n",
      "  Meltdown:               Not affected\n",
      "  Mmio stale data:        Unknown: No mitigations\n",
      "  Reg file data sampling: Not affected\n",
      "  Retbleed:               Not affected\n",
      "  Spec rstack overflow:   Not affected\n",
      "  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n",
      "                          l\n",
      "  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n",
      "                          r sanitization\n",
      "  Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditiona\n",
      "                          l; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, \n",
      "                          KVM SW loop\n",
      "  Srbds:                  Not affected\n",
      "  Tsx async abort:        Mitigation; TSX disabled\n"
     ]
    }
   ],
   "source": [
    "# check CPU details\n",
    "! lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4de6562-aad0-4374-a674-470a86ae665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  7 10:56:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 PCIe               On  |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             48W /  350W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check GPU details\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba34833-3574-421b-81f7-bd2fd38190cd",
   "metadata": {},
   "source": [
    "NeMo Curator provides a simple function `get_client` that can be used to start a local Dask cluster or connect to an existing one.  Let's initialize the Dask Cluster. \n",
    "\n",
    "The next cell starts a Dask `LocalCluster` on your CPU. It can be reused for all modules except for deduplication, which requires a GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7f130-88b8-4e2c-b2f0-1a26bcfebcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "\n",
    "client = get_client(cluster_type=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d3c8c-df05-4291-a17e-beda92da813f",
   "metadata": {},
   "source": [
    "Lear more about Nemo Curator's CPU and GPU Modules with Dask in the dedicated [documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/cpuvsgpu.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebab4e1-a211-4845-82b3-f779defda8b1",
   "metadata": {},
   "source": [
    "## 1.1 Multilingual Datasets\n",
    "\n",
    "In this notebook, we will use a subset of the [MC4](https://huggingface.co/datasets/allenai/c4), the C4 Multilingual Dataset.\n",
    "\n",
    "For the sake of this exercice, to create a more diverse dataset:\n",
    "- We merged Spanish and French samples (100 per language)\n",
    "- We duplicated all samples (making 200 samples per language)\n",
    "- We shuffled the samples\n",
    "\n",
    "So, we have 400 samples, 200 from each language. The structure is a JSON format with 3 filed: `text`, `timestamp` and `url`. \n",
    "\n",
    "Let's have a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8427d4-8e4d-4731-8e5b-b417f9165f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset file path\n",
    "multilingual_data = \"./datasets/file.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271915b3-498f-4445-a415-05d8b599e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 ./datasets/file.json\n"
     ]
    }
   ],
   "source": [
    "# check number of samples\n",
    "! wc -l {multilingual_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852ec88c-5ac8-40c8-a0c2-3647ec086182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Dragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser sur Buzz, insolite et culture\\nDragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser\\nLe 20e film Dragon Ball sortira le vendredi 14 décembre 2018. La première affiche teaser montre un Gokû jeune adulte, environ celui de la fin de Dragon Ball et le début de Dragon Ball Z. À lire aussi >>> Le gouvernement mexicain prévoit la diffusion sur place publique des épisodes 130 et 131 de Dragon […]...\\nLire la suite du buzz sur bleachmx Source : bleachmx - 12/03/2018 22:31 - trending_up142\\nfilm commémoration Akira Toriyama dbz dragon ball Dragon Ball Super dragon ball z affiche Dragon Ball Super Anime V Jump Décembre 2018 Dragon Ball Z Battle of Gods Dragon Ball Z Fukkatsu No [F] Dragon Ball Z La Résurrection de [F] Potins Films\\nLe site Deadline indique ce Jeudi que le film d’animation Dragon Ball Super – Broly a rapporté, selon les estimations, plus de 7 millions de dollars pour sa première journée d’exploitation aux États-Unis. Il prévoit que le film rapporte plus de 15 millions de dollars pour sa première semaine. À voir aussi >>> Dragon Ball […] ...\\nSource : bleachmx - 18/01/2019 00:16 - trending_up 15\\nDragon Ball Super Broly: Le film d’animation prévu dans 90 pays - bleachmx\\nDragon Ball Super – Broly: Le film adapté en manga et en light novel - bleachmx\\nLors du DRAGON BALL Games SUPER Showcase il a été annoncé que le je vidéo Super Dragon Ball Heroes: World Mission sortira sur Nintendo Switch et PC (via Steam) le 5 avril 2019 en occident. Le trailer sous-titré a été présenté ainsi qu’une vidéo de gameplays. La vidéo présente Cirrus (Shiirus – Shiirasu), le nouveau […] ...\\nSource : bleachmx - 15/01/2019 01:45 - trending_up 22\\nSuper Dragon Ball Heroes: World Mission : Deuxième trailer du jeu vidéo - bleachmx\\nDes avants-premières pour le film Dragon Ball Super Broly dans les cinémas CGR\\nLe film Dragon Ball Super Broly est attendu comme le messie par les fans de l'univers d'Akira Toriyama. Après les annonces d'avants-premières au ...\\nSource : manga-news - 08/01/2019 11:00 - trending_up 15\\nLes avants-premières françaises du film Dragon Ball Super Broly dévoilées\\nParticulièrement attendu, le film Dragon Ball Super Broly sortira dans les cinémas de France le 13 mars. Mais avant ça, plusieurs avant-premières ...\\nSource : manga-news - 04/01/2019 15:16 - trending_up 16\\nAvant-premi?res de Dragon Ball Super Broly dans toute la France (Les 23 et 24 janvier 2019)\\nLes 23 et 24 janvier 2019 Le film Dragon Ball Super Broly est sorti au Japon le 13 décembre 2018 et paraîtra également au cinéma en France le 13 mars 2019. Avant cette date, plusieurs avant-premières son programmés en janvier. Mercredi 23 et jeudi 24 janvier Le Grand Rex (Paris) Jeudi 24 janvier Pathé : St Herblain, Pathé Nantes-Atlantis (44, Loire-Atlantique) Labège, Gaumont (31, Haute-Garonne) Toulouse, Gaumont Wilson ( 31, Haute-Garonne) Belle-Epine, Pathé (94, Thiais) ... ...\\nSource : animint - 04/01/2019 14:46 - trending_up 26\\nOfficiel : le film Dragon Ball Super : Broly en France le 13 mars (VOSTFR et VF) ! - animeland\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 - bleachmx\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 ? - bleachmx\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l’épisode 7\\nLe site officiel de l’anime promotionnel Super Dragon Ball Heroes: Universal Mission a mis en ligne une affiche, un synopsis ainsi qu’un teaser vidéo pour l’épisode 7. L’épisode 7 sortira le 10 janvier 2019. À voir aussi >>> Dragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records […] ...\\nSource : bleachmx - 30/12/2018 00:02 - trending_up 39\\nSuper Dragon Ball Heroes : Épisode 5, preview date de sortie de l’épisode 6 - bleachmx\\nLe jeu Super Dragon Ball Heroes : World Mission daté en Occident - manga-news\\nDragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records au box office japonais\\nLe journal Mainichi Shimbun a annoncé dans ses pages que le film d’animation Dragon Ball Super: Broly a rapporté 2 milliards de yens (18,1 millions de dollars $) en 11 jours au box office japonais. Il est le film de la franchise a avoir atteint le plus rapidement la barre des 2 milliards de yens. […] ...\\nSource : bleachmx - 26/12/2018 17:46 - trending_up 15\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l’épisode 7 - bleachmx\\nDragon Ball Super – Broly: Un Vegeta enfin respecté, les premières minutes émouvantes et l’avant-première mondiale - bleachmx\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon\\nLe webjournal japonais Mantan Web a révélé que le film animation Dragon Ball Super Broly a généré 2 milliards de yen de recettes (15,9 millions €) en 11 jours ! ...\\nSource : adala-news - 25/12/2018 11:01 - trending_up 43\\nLe film animation Dragon Ball Super Broly, en Trailer 2 - adala-news\\nLe film animation Dragon Ball Super Broly, en Trailer 3 - adala-news\\nTop 5 des films animation Dragon Ball les plus populaires au Japon\\nLa Toei a dévoilé le classement des films animation Dragon Ball préférés des japonais ! ...\\nSource : adala-news - 24/12/2018 00:03 - trending_up 28\\nPremiers chiffres du film animation Dragon Ball Super Broly au Japon - adala-news\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon - adala-news\\nDragon Ball Super Chapitre Scan 043 VF\\nLe chapitre 43 de Dragon Ball Super pour ce mois de décembre 2018 pour clôturer l’année. Le film Dragon Ball Super – Broly est enfin sorti au Japon et le manga entame l’Arc du Prisonnier de la Patrouille Galactique. C’est donc une nouvelle fois le film qui fait la couverture du magazine. Toutes les pages […] ...\\nSource : bleachmx - 21/12/2018 04:00 - trending_up 38\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1548042730000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"https://cultinfos.com/buzz/332814-dragon-ball-20e-film-de-sage-sortira-14-decembre-premiere-image-teaser\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Cours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition) ➠ Maximilien Samson Frederic Schoell | Буквоед ISBN 978-5-8792-8565-9\\nCours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\nISBN: 978-5-8792-8565-9\\nКод: pod 1702886\\nАвторы: Samson, Schoell\\nС товаром «Cours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)» часто покупают\\nЕсли Вы обнаружили ошибку в описании товара «Cours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)» Maximilien Samson Frederic Schoell, выделите её мышкой и нажмите: Ctrl+Enter. Спасибо!\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1547767539000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"https://www.bookvoed.ru/book?id=1433688\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Se realizó una jornada de promoción del buentrato hacia los adultos mayores en Río Gallegos - Ministerio de Desarrollo Social\\nSe realizó una jornada de promoción del buentrato hacia los adultos mayores en Río Gallegos\\nFue el fin de semana pasado en la capital santacruceña. El sábado tuvo lugar una capacitación sobre deporte social. El domingo más de 350 personas participaron de actividades recreativas, charlas y talleres.\\nSe realizó la jornada de promoción del buentrato hacia los adultos mayores en Río Gallegos.\\nFue el fin de semana pasado en el SUM de Vialidad Nacional de la capital santacruceña.\\nEl sábado tuvo lugar una capacitación sobre deporte social.\\nEl domingo más de 350 personas participaron de actividades recreativas, charlas y talleres.\\nEl Ministerio de Desarrollo Social a través del Centro de Referencia de Santa Cruz (CDR) llevó a cabo una jornada de promoción del buentrato hacia los adultos mayores en la ciudad de Río Gallegos. Las actividades se desarrollaron durante el fin de semana pasado en el marco del “Día mundial contra el abuso y el maltrato a los mayores”.\\nEl sábado, desde el proyecto “Madurar en Positivo” perteneciente al Plan Nacional de Deporte Social, se realizó una capacitación en el salón de usos múltiples de Vialidad Nacional dirigida a más de 50 ciudadanos. Asimismo el domingo, más de 350 adultos mayores participaron de actividades recreativas y disfrutaron de juegos de kermés, tejo, sapo, bingo, ping pong, vóleibol adaptado, bowling y talleres.\\nLa Subsecretaría de Responsabilidad Social brindó una charla para concientizar acerca de la importancia de reducir los factores de riesgo asociados a la vida sedentaria, promoviendo hábitos saludables para mejorar la calidad de vida. Por su parte, la Dirección de Deporte Social llevó a cabo el “taller de la risa, donde a través de técnicas de juegos la directora Nacional, Patricia Borrillo habló de las herramientas para que las personas mayores aprendan a distanciarse de las preocupaciones y cuenten con distintas alternativas ante la resolución de un problema.\\nA su vez, los presentes pudieron acceder de manera gratuita, a una unidad móvil sanitaria del programa “Argentina Sonríe” del Ministerio de Salud de Nación. También se hicieron controles de glucemia, presión y vacunación. Por la tarde, se presentaron diversos números artísticos para compartir en familia: el grupo Papelnonos, el coro del Centro de Jubilados “El Despertar” y el grupo de danzas del Centro de Jubilados “La Amistad” y del centro “Encuentro de Amigos”.\\nAl finalizar la jornada, el referente del CDR local, Ariel Fernández, destacó las acciones conjuntas entre todos los organismos presentes, afirmando que la actividad no tiene que ver con el trabajo de un ministerio sino con “un proyecto político con una mirada integral”.\\nLa cartera social a través de los CDR y en articulación con todos los organismos del Gobierno nacional, concibe a las personas mayores como protagonistas del cambio social y promotores de la cultura del buentrato. De esta manera, se trabajan acciones diarias en pos de mejorar la calidad de vida de los adultos mayores.\\n\\\"Sean transgresores, no le digan que sí a todo. Acuérdense la experiencia no se jubila. El desafío es que todos los adultos mayores sean sujetos activos de derecho\\\".\\nLos mayores enseñan a los chicos los juegos de su infancia\\nEl Hogar Balestra cumplió 90 años\\nConocé más sobre Adultos mayores\\nEncontrá dónde consultar sobre Adultos mayores\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1524296308000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"http://www.desarrollosocial.gob.ar/noticias/se-realizo-una-jornada-de-promocion-del-buentrato-hacia-los-adultos-mayores-en-rio-gallegos/\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# show the 3 first samples\n",
    "! head -n 3 {multilingual_data} | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d22236-bc9e-4668-ae3c-d35bdbf6f5de",
   "metadata": {},
   "source": [
    "Notice, **languages are not annotated in the dataset**, allowing us to leverage AI-based language separation later in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f276db-4372-4775-b0a0-08b94fe5c3f9",
   "metadata": {},
   "source": [
    "Let's now create a document dataset from a pandas data frame. For more information on the arguments see Dask’s from_pandas documentation\n",
    "\n",
    "NeMo Curator's `DocumentDataset` employs Dask's distributed dataframes to mangage large datasets across multiple nodes and allow for easy restarting of interrupted curation. `DocumentDataset` supports reading and writing to sharded *jsonl* and *parquet* files both on local disk and from remote sources such as S3 bukets.\n",
    "\n",
    "Let's load our multilingual dataset with Nemo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d2d25e-5a7d-4639-9e17-bf09759726c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e230e1fb-898a-47e1-84d0-a2578cd01a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./datasets/multilingual\"\n",
    "multilingual_dataset = DocumentDataset.read_json(\n",
    "    multilingual_data_path, add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89f475e-0284-4681-aec1-a536e0bd751c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DocumentDataset' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmultilingual_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DocumentDataset' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "multilingual_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1ef92-efd2-434b-96d3-2722bcfcc955",
   "metadata": {},
   "source": [
    "## 1.2 Basic Text cleaning and Unification\n",
    "\n",
    "NeMo Curator provides various `DocumentModifier` implementations such as the `UnicodeReformatter` which uses [ftfy](https://pypi.org/project/ftfy/) (fixes text for you) to resolve all unicode issues in the dataset. \n",
    "\n",
    "It is also possible to implement your custom text cleaner using `DocumentModifier`. For instance, we can standardize inconsistent quotation marks that appear very often in curated large dataset, remove HTML, URLs, and email tags, etc.\n",
    "\n",
    "\n",
    "Let's first create the output folders to save the cleaned step outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4df995-300b-470d-847b-78839ca859b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set dataset file path\n",
    "curated_data_path = \"./curated\"\n",
    "clean_and_unify_data_path = os.path.join(curated_data_path, \"01_clean_and_unify\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(curated_data_path, exist_ok=True)\n",
    "os.makedirs(clean_and_unify_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb4a71-2347-4cd3-a5e9-0711434a467d",
   "metadata": {},
   "source": [
    "Let's now implement a custom text cleaner `QuotationTagUnifier`.\n",
    "\n",
    "It is designed to modify text documents by normalizing quotation marks and removing unwanted elements. \n",
    "\n",
    "The result is a cleaned and standardized text output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dc7a4-d9e7-464d-864b-baa653cab173",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd0a641-4063-4781-8d0d-e5e697ad50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import dask\n",
    "import pandas as pd\n",
    "from nemo_curator.modifiers import DocumentModifier, UnicodeReformatter\n",
    "from nemo_curator.modules.modify import Modify\n",
    "\n",
    "\n",
    "class QuotationTagUnifier(DocumentModifier):\n",
    "    def modify_document(self, text: str) -> str:\n",
    "        text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = re.sub(\n",
    "            r\"(<[^>]+>)|(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\",\n",
    "            \"\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838a49-15d8-4bfd-b5ea-c6c7d09652ba",
   "metadata": {},
   "source": [
    "Next, we can chain modifiers together using the `Sequential` class, which takes a list of operations to be run sequentially and applies them to a given `DocumentDataset`.ipynb_checkpoints/\n",
    " \n",
    "Let's call this sequence the `cleaners`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35cd6bbb-0be0-4089-88d7-e46ca1d21c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import Sequential\n",
    "\n",
    "cleaners = Sequential(\n",
    "    [\n",
    "        # Apply: Unify all the quotation marks and remove tags\n",
    "        Modify(QuotationTagUnifier()),\n",
    "        # Apply: Unify all unicode\n",
    "        Modify(UnicodeReformatter()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43d8e7-a892-46e8-9719-1ea2363d6577",
   "metadata": {},
   "source": [
    "Let's run that on a toy example with few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf51cce-2ff4-4ded-a54d-a12773a8074f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DocumentDataset' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset_toy \u001b[38;5;241m=\u001b[39m DocumentDataset(dask\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39mfrom_pandas(dataframe_toy, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# check the samples\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdataset_toy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DocumentDataset' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# create the toy samples\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play ‘footbal’\",\n",
    "            \"He is very  \\t  happy.\",\n",
    "            \"Visit <a href='www.example.com'>example.com</a> for more information or contact us at info@example.com\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "# check the samples\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705224b3-31d4-4702-97a6-8e2f572b7a6f",
   "metadata": {},
   "source": [
    "Now, let's apply our sequence of cleaners to the toy samples. To execute this sequence on the Dask cluster, we use `.persist()`, which keeps the transformed data in memory for optimized processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc070516-28b6-425d-a9b6-d3792f3ed8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_clean_and_unify = cleaners(dataset_toy).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bc55b-3a28-4825-baa3-cea46491c084",
   "metadata": {},
   "source": [
    "Let's check the output.\n",
    "\n",
    "Expected output are samples with normalized quotations, removed tabs and HTML, URL and Email tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78c5a8-4548-4106-8f4d-cab9ffc18eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cleaned toy samples\n",
    "dataset_test_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52122f-9a73-421a-a3b7-0b4dd234cb2b",
   "metadata": {},
   "source": [
    "Now, let's apply this cleaning step to our multilingual dataset. We can achieve this by creating a sequence of curation steps, starting with the cleaning sequence as the first function in our data curation pipeline.\n",
    "\n",
    "Run the next cell to create the cleaning step as a function that would be the first curation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e999870-cbe9-4d28-8b02-8d6e6a8f80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sequence of cleaning operations as a function\n",
    "def clean_and_unify(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    cleaners = Sequential(\n",
    "        [\n",
    "            # Apply: Unify all the quotation marks and remove tags\n",
    "            Modify(QuotationTagUnifier()),\n",
    "            # Apply: Unify all unicode\n",
    "            Modify(UnicodeReformatter()),\n",
    "        ]\n",
    "    )\n",
    "    return cleaners(dataset)\n",
    "\n",
    "\n",
    "# sequence of data curation setps. so far, only cclean_and_unify is defined\n",
    "curation_steps = Sequential([clean_and_unify])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af0dae-640c-48ad-9023-25d19a4caa80",
   "metadata": {},
   "source": [
    "Let's now execute this step on out multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d03f50-ddfa-4cb7-86bf-e06f9bd70c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Executing the pipeline...\")\n",
    "\n",
    "dataset_clean_and_unify = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee639b-99d7-4026-a87a-90c8dd960335",
   "metadata": {},
   "source": [
    "Let's check some outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc175867-7d83-4452-98df-f79bce5d1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2abe75-18db-44eb-b881-be16d046fb47",
   "metadata": {},
   "source": [
    "We can save the created Document into a json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14fede-e581-4235-81c3-7ace79a593a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output to json\n",
    "dataset_clean_and_unify.to_json(clean_and_unify_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea2e91-550d-494a-8794-533531c341c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 {clean_and_unify_data_path}/file.jsonl | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89228aa9-641f-4a1d-96b5-61fccb703832",
   "metadata": {},
   "source": [
    "## Dataset document size Filtering\n",
    "\n",
    "Extremely short documents may lack sufficient context or information for the model to learn meaningful concepts. By filtering out such documents, we can ensure that the data used for training is sufficiently informative and balanced.\n",
    "\n",
    "Let's explore how to apply word counts and filtering using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59e1ac-4a33-4305-abe6-25d3e3ab596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import (\n",
    "    DocumentFilter,\n",
    "    RepeatingTopNGramsFilter,\n",
    "    WordCountFilter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f424f0-2db3-47b4-a919-666aea19d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncompleteDocumentFilter(DocumentFilter):\n",
    "    \"\"\"\n",
    "    If the document doesn't end with a terminating punctuation mark, then discard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Accepted document terminators.\n",
    "        self._story_terminators = {\".\", \"!\", \"?\", '\"', \"”\"}\n",
    "\n",
    "    def score_document(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a document's score is valid based on the last character of the text.\n",
    "        Args:\n",
    "            text (str): The document text.\n",
    "        Returns:\n",
    "            bool: True if the document's score is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return text.strip()[-1] in self._story_terminators\n",
    "\n",
    "    def keep_document(self, score) -> bool:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402e2b-9b38-4a0d-877b-a62a935f1c89",
   "metadata": {},
   "source": [
    "The following code defines a function, `filter_dataset`, that cleans a `DocumentDataset` by applying several filters:\n",
    "\n",
    "- **Word Count Filter**: Removes documents with fewer than 80 words by default.\n",
    "- **Incomplete Document Filter**: Removes incomplete documents.\n",
    "- **Repeating N-Grams Filters**: Removes documents with excessive repetition of word sequences (2-grams, 3-grams, 4-grams) above certain thresholds (20%, 18%, 16% respectively).\n",
    "\n",
    "These filters are applied sequentially to refine the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610d1cb-f44f-42fc-bb37-8759a50b1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    filters = Sequential(\n",
    "        [\n",
    "            ScoreFilter(\n",
    "                WordCountFilter(min_words=80),\n",
    "                text_field=\"text\",\n",
    "                score_field=\"word_count\",\n",
    "            ),\n",
    "            ScoreFilter(IncompleteDocumentFilter(), text_field=\"text\"),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=2, max_repeating_ngram_ratio=0.2),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=3, max_repeating_ngram_ratio=0.18),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=4, max_repeating_ngram_ratio=0.16),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return filters(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee9ddd-57b3-4cac-8351-fb8b7fb5acef",
   "metadata": {},
   "source": [
    "Let's now apply that on our multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef91e32-c9ea-4bf4-b1ab-016ee9413e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "filtered_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0c2dd-0e63-4105-aeee-a41b12f0fb06",
   "metadata": {},
   "source": [
    "We can check the outputs. Notice that a new field named `word_count` has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e03f52-5757-41b9-9e09-05c3cb5e67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b41e2-79a7-4a5e-a823-1aa07d29926b",
   "metadata": {},
   "source": [
    "Let's save the output, we need to create the folder first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7840e33-7414-4819-99bc-60e3df51c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_path = os.path.join(curated_data_path, \"02_filter_dataset\")\n",
    "\n",
    "! mkdir -p {filtered_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80a5b3-8542-42a7-b818-3390653702f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output to json\n",
    "filtered_dataset.to_json(filtered_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4874-8107-41f0-8358-971011b17fe1",
   "metadata": {},
   "source": [
    "Check the saved file by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89490af-abab-44ab-919e-9c2847f9384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 {filtered_data_path}/file.jsonl | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dd4da-9f59-454c-a4c5-725d8cc2a1db",
   "metadata": {},
   "source": [
    "### 1.3 PII Identification and Removal\n",
    "\n",
    "The Personal Identifiable Information (PII) identification tool is designed to remove sensitive data from datasets.\n",
    "\n",
    "The identification leverages [presidio_analyzer](https://pypi.org/project/presidio-analyzer/) a Python based service for detecting PII entities in text.\n",
    "\n",
    "Let's try to analyze a toy sample: *My name is Dana and my number is 212-555-5555*\n",
    "\n",
    "Expected output is the type `PERSON` and `PHONE_NUMBER` and the char start and end position.\n",
    "\n",
    "```\n",
    " type: PERSON, start: 11, end: 15, score: 0.85,\n",
    " type: PHONE_NUMBER, start: 33, end: 45, score: 0.75\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaef24e-c838-408c-9242-f40cfc11146d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "# Hide deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\n",
    "LANGUAGES_CONFIG_FILE = \"./languages-config.yml\"\n",
    "# Create NLP engine based on configuration file\n",
    "provider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "analyzer = AnalyzerEngine(\n",
    "    supported_languages=[\"en\", \"es\", \"fr\"], nlp_engine=nlp_engine_with_spanish\n",
    ")\n",
    "\n",
    "results = analyzer.analyze(\n",
    "    text=\"My name is Dana and my number is 212-555-5555\",\n",
    "    entities=[\"PHONE_NUMBER\", \"PERSON\"],\n",
    "    language=\"en\",\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a163574-b7e2-4100-a054-ec53c0a2ff89",
   "metadata": {},
   "source": [
    "Run the analyzer for French sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f2672-9671-4fde-8ac7-270b4c81cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.analyze(text=\"Mon email est mon@example.com\", language=\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfef904-5323-411f-9b04-7fc8cbf95f81",
   "metadata": {},
   "source": [
    "Try your own examples in these three languages for accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f9d69-1563-48a7-b2ea-4299489c62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = # Your code here\n",
    "analyzer.analyze(text=input, language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa84a63-5062-44bc-b948-f85f246d7e42",
   "metadata": {},
   "source": [
    "Nemo Curator integrates PII Identification and Removal efficiently leveraging Dask for parallelization. The tool currently supports the identification and removal of the following sensitive data types:\n",
    "\n",
    "`ADDRESS`,`CREDIT_CARD`,`EMAIL_ADDRESS`,`DATE_TIME`,`IP_ADDRESS`,`LOCATION`,`PERSON`,`URL`,`US_SSN`,`US_PASSPORT`,`US_DRIVER_LICENSE`,`PHONE_NUMBER`,\n",
    "\n",
    "Let;s run the Nemo Curator PII Identification `PiiModifier` on a toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2cc51-7177-4224-a637-5c167c6f7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy samples with PII data\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play football\",\n",
    "            \"His email is ryan@example.com and phone is 212-555-5555\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3f12d-ef3f-48a5-bd1a-1902de2d8c3d",
   "metadata": {},
   "source": [
    "Let's build and apply the `PiiModifier` on the toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b52e11-bfd9-4aa5-b721-cff7e67ff6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "modifier = PiiModifier(\n",
    "    batch_size=2000,\n",
    "    language=\"en\",\n",
    "    supported_entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],\n",
    "    anonymize_action=\"replace\",\n",
    ")\n",
    "\n",
    "modify = Modify(modifier)\n",
    "modified_dataset = modify(dataset_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071a86f9-1e93-47e9-a1f8-e88f182bc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check modified data\n",
    "modified_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1654a77-4330-4902-99bf-056be4920dcb",
   "metadata": {},
   "source": [
    "Now, let's integrate this PII identification step into our curation sequence and apply it to the multilingual dataset. This will ensure that sensitive data is properly detected and removed while maintaining data quality. \n",
    "\n",
    "Let's create first the `redact_pii` function for PII identification and removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d0f90-6efa-4ea9-a9e4-c1849e0a637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "\n",
    "def redact_pii(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    redactor = Modify(\n",
    "        PiiModifier(\n",
    "            supported_entities=[\"PERSON\"],\n",
    "            anonymize_action=\"replace\",\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return redactor(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622c883-8fb5-4951-84e7-9f0a80a7ce29",
   "metadata": {},
   "source": [
    "Let's now run the sequence of curation steps including the PII removal function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c780b-2632-4ff9-9ddd-7a889bf1f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset, redact_pii])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "redact_pii_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce773a-9520-44a9-bad4-9915d8a808d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the filtered data\n",
    "redact_pii_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e50d88-8a61-46c0-b42f-94a22a3ad4d3",
   "metadata": {},
   "source": [
    "Let's now save the fileted data. We need to create the folder to save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b610ca1-5d53-4d00-aa78-60c8e019bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "redact_pii_data_path = os.path.join(curated_data_path, \"03_redact_pii_data_path\")\n",
    "\n",
    "! mkdir -p {redact_pii_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8259bb6-64ac-46d7-9813-de0e06ec5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "redact_pii_dataset.to_json(redact_pii_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb0bc4-b310-4bb8-8a3d-a2f3e8806296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the saved file\n",
    "! head -n 1 {redact_pii_data_path}/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f0d3c-4646-4a24-9e86-de0bc153111d",
   "metadata": {},
   "source": [
    "The current PII removal s Nemo Curator implementation is limited to HPC clusters using Slurm as the resource manager. Check the [documentation](https://github.com/NVIDIA/NeMo-Curator/blob/main/docs/user-guide/personalidentifiableinformationidentificationandremoval.rst) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1f1a1-55e1-407b-a494-c17bb58c9258",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have used Nemo Curator to apply a sequence of basic text curation steps designed to clean and preprocess the dataset.\n",
    "\n",
    "Before moving on to the next notebook, make sure to stop the Dask cluster. Please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b44b3b-5db7-4a3f-929a-c3311eb5e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74536337-a5a3-4e54-9f9c-f7049b7f684e",
   "metadata": {},
   "source": [
    "\n",
    "We are now ready to move to the next notebook to explore advanced data preparation steps. \n",
    "\n",
    "Let's move to the [02_advanced_data_processing](02_advanced_data_processing.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1e5e3-60b1-47d7-87e3-96c8d73e99e0",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
