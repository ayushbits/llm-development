{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0936ea03-3f38-49e9-91c3-2d370769c8a8",
   "metadata": {},
   "source": [
    "# 1. Basics of Data Curation\n",
    "\n",
    "******\n",
    "\n",
    "Generative AI developemet requires a havy data curation process. The quality the model largely depends on the quality of the data used for training. NVIDIA NeMo Curator is an open-source framework designed to streamline this process by preparing large-scale, high-quality datasets for pretraining and continuous training.\n",
    "\n",
    "NeMo Curator offers built-in workflows for curating data from various public sources such as Common Crawl, Wikipedia, and arXiv. At the same time, it provides the flexibility to customize pipelines to suit the specific needs of your project.\n",
    "\n",
    "This notebook guides the process of basic data preparation involved in most Language Models developements: \n",
    "\n",
    "**[1.1 Text Cleaning and Unification](#1.1-Text-Cleaning-and-Unification)<br>**\n",
    "**[1.2 Document Size Filtering](#1.2-Document-Size-Filtering)<br>**\n",
    "**[1.3 Filter Personally Identifiable Information (PII)](#1.3-Filter-Personally-Identifiable-Information-(PII))<br>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfc4b2-8fd0-4cbf-86ae-094e6c8fead9",
   "metadata": {},
   "source": [
    "***************\n",
    "### Environment Setup\n",
    "\n",
    "For large-scale data processing, NeMo Curator provides both GPU and CPU based modules. Understanding how these modules interact and how to configure your environment is key to optimizing performance.\n",
    "\n",
    "CPU-based modules rely on [Dask](https://www.dask.org/) to distribute computations across multi-node clusters while GPU-accelerated modules uses [RAPIDS](https://rapids.ai/) to handle large-scale datasets efficiently.\n",
    "\n",
    "Let's check first your current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652486cf-06ef-43af-b136-96cda3b7714c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:             x86_64\n",
      "  CPU op-mode(s):         32-bit, 64-bit\n",
      "  Address sizes:          52 bits physical, 57 bits virtual\n",
      "  Byte Order:             Little Endian\n",
      "CPU(s):                   26\n",
      "  On-line CPU(s) list:    0-25\n",
      "Vendor ID:                GenuineIntel\n",
      "  Model name:             Intel(R) Xeon(R) Platinum 8480+\n",
      "    CPU family:           6\n",
      "    Model:                143\n",
      "    Thread(s) per core:   2\n",
      "    Core(s) per socket:   13\n",
      "    Socket(s):            1\n",
      "    Stepping:             8\n",
      "    BogoMIPS:             4000.00\n",
      "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n",
      "                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysc\n",
      "                          all nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep\n",
      "                          _good nopl xtopology cpuid tsc_known_freq pni pclmulqd\n",
      "                          q vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic mo\n",
      "                          vbe popcnt tsc_deadline_timer aes xsave avx f16c rdran\n",
      "                          d hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssb\n",
      "                          d ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriorit\n",
      "                          y ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep b\n",
      "                          mi2 erms invpcid avx512f avx512dq rdseed adx smap avx5\n",
      "                          12ifma clflushopt clwb avx512cd sha_ni avx512bw avx512\n",
      "                          vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16\n",
      "                           wbnoinvd arat vnmi avx512vbmi umip pku ospke waitpkg \n",
      "                          avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_b\n",
      "                          italg avx512_vpopcntdq la57 rdpid bus_lock_detect clde\n",
      "                          mote movdiri movdir64b fsrm md_clear serialize tsxldtr\n",
      "                          k avx512_fp16 arch_capabilities\n",
      "Virtualization features:  \n",
      "  Virtualization:         VT-x\n",
      "  Hypervisor vendor:      KVM\n",
      "  Virtualization type:    full\n",
      "Caches (sum of all):      \n",
      "  L1d:                    832 KiB (26 instances)\n",
      "  L1i:                    832 KiB (26 instances)\n",
      "  L2:                     52 MiB (13 instances)\n",
      "  L3:                     16 MiB (1 instance)\n",
      "NUMA:                     \n",
      "  NUMA node(s):           1\n",
      "  NUMA node0 CPU(s):      0-25\n",
      "Vulnerabilities:          \n",
      "  Gather data sampling:   Not affected\n",
      "  Itlb multihit:          Not affected\n",
      "  L1tf:                   Not affected\n",
      "  Mds:                    Not affected\n",
      "  Meltdown:               Not affected\n",
      "  Mmio stale data:        Unknown: No mitigations\n",
      "  Reg file data sampling: Not affected\n",
      "  Retbleed:               Not affected\n",
      "  Spec rstack overflow:   Not affected\n",
      "  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n",
      "                          l\n",
      "  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n",
      "                          r sanitization\n",
      "  Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditiona\n",
      "                          l; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, \n",
      "                          KVM SW loop\n",
      "  Srbds:                  Not affected\n",
      "  Tsx async abort:        Mitigation; TSX disabled\n"
     ]
    }
   ],
   "source": [
    "# check CPU details\n",
    "! lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4de6562-aad0-4374-a674-470a86ae665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  7 18:02:13 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   32C    P0            122W /  700W |     531MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           32271      C   /opt/venv/bin/python                    522MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check GPU details\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba34833-3574-421b-81f7-bd2fd38190cd",
   "metadata": {},
   "source": [
    "NeMo Curator provides a simple function `get_client` that can be used to start a local Dask cluster or connect to an existing one.  Let's initialize the Dask Cluster. \n",
    "\n",
    "The next cell starts a Dask `LocalCluster` on your CPU. It can be reused for all modules except for deduplication, which requires a GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f7f130-88b8-4e2c-b2f0-1a26bcfebcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:18: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from optuna import progress_bar as pbar_module\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "\n",
    "client = get_client(cluster_type=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d3c8c-df05-4291-a17e-beda92da813f",
   "metadata": {},
   "source": [
    "Lear more about Nemo Curator's CPU and GPU Modules with Dask in the dedicated [documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/cpuvsgpu.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebab4e1-a211-4845-82b3-f779defda8b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Multilingual Datasets\n",
    "\n",
    "In this notebook, we will use a subset of the [MC4](https://huggingface.co/datasets/allenai/c4), the C4 Multilingual Dataset.\n",
    "\n",
    "For the sake of this exercice, to create a more diverse dataset:\n",
    "- We merged Spanish and French samples (100 per language)\n",
    "- We duplicated all samples (making 200 samples per language)\n",
    "- We shuffled the samples\n",
    "\n",
    "So, we have 400 samples, 200 from each language. The structure is a JSON format with 3 filed: `text`, `timestamp` and `url`. \n",
    "\n",
    "Let's have a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8427d4-8e4d-4731-8e5b-b417f9165f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset file path\n",
    "multilingual_data = \"./datasets/multilingual/file.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "271915b3-498f-4445-a415-05d8b599e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 ./datasets/multilingual/file.json\n"
     ]
    }
   ],
   "source": [
    "# check number of samples\n",
    "! wc -l {multilingual_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852ec88c-5ac8-40c8-a0c2-3647ec086182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Dragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser sur Buzz, insolite et culture\\nDragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser\\nLe 20e film Dragon Ball sortira le vendredi 14 décembre 2018. La première affiche teaser montre un Gokû jeune adulte, environ celui de la fin de Dragon Ball et le début de Dragon Ball Z. À lire aussi >>> Le gouvernement mexicain prévoit la diffusion sur place publique des épisodes 130 et 131 de Dragon […]...\\nLire la suite du buzz sur bleachmx Source : bleachmx - 12/03/2018 22:31 - trending_up142\\nfilm commémoration Akira Toriyama dbz dragon ball Dragon Ball Super dragon ball z affiche Dragon Ball Super Anime V Jump Décembre 2018 Dragon Ball Z Battle of Gods Dragon Ball Z Fukkatsu No [F] Dragon Ball Z La Résurrection de [F] Potins Films\\nLe site Deadline indique ce Jeudi que le film d’animation Dragon Ball Super – Broly a rapporté, selon les estimations, plus de 7 millions de dollars pour sa première journée d’exploitation aux États-Unis. Il prévoit que le film rapporte plus de 15 millions de dollars pour sa première semaine. À voir aussi >>> Dragon Ball […] ...\\nSource : bleachmx - 18/01/2019 00:16 - trending_up 15\\nDragon Ball Super Broly: Le film d’animation prévu dans 90 pays - bleachmx\\nDragon Ball Super – Broly: Le film adapté en manga et en light novel - bleachmx\\nLors du DRAGON BALL Games SUPER Showcase il a été annoncé que le je vidéo Super Dragon Ball Heroes: World Mission sortira sur Nintendo Switch et PC (via Steam) le 5 avril 2019 en occident. Le trailer sous-titré a été présenté ainsi qu’une vidéo de gameplays. La vidéo présente Cirrus (Shiirus – Shiirasu), le nouveau […] ...\\nSource : bleachmx - 15/01/2019 01:45 - trending_up 22\\nSuper Dragon Ball Heroes: World Mission : Deuxième trailer du jeu vidéo - bleachmx\\nDes avants-premières pour le film Dragon Ball Super Broly dans les cinémas CGR\\nLe film Dragon Ball Super Broly est attendu comme le messie par les fans de l'univers d'Akira Toriyama. Après les annonces d'avants-premières au ...\\nSource : manga-news - 08/01/2019 11:00 - trending_up 15\\nLes avants-premières françaises du film Dragon Ball Super Broly dévoilées\\nParticulièrement attendu, le film Dragon Ball Super Broly sortira dans les cinémas de France le 13 mars. Mais avant ça, plusieurs avant-premières ...\\nSource : manga-news - 04/01/2019 15:16 - trending_up 16\\nAvant-premi?res de Dragon Ball Super Broly dans toute la France (Les 23 et 24 janvier 2019)\\nLes 23 et 24 janvier 2019 Le film Dragon Ball Super Broly est sorti au Japon le 13 décembre 2018 et paraîtra également au cinéma en France le 13 mars 2019. Avant cette date, plusieurs avant-premières son programmés en janvier. Mercredi 23 et jeudi 24 janvier Le Grand Rex (Paris) Jeudi 24 janvier Pathé : St Herblain, Pathé Nantes-Atlantis (44, Loire-Atlantique) Labège, Gaumont (31, Haute-Garonne) Toulouse, Gaumont Wilson ( 31, Haute-Garonne) Belle-Epine, Pathé (94, Thiais) ... ...\\nSource : animint - 04/01/2019 14:46 - trending_up 26\\nOfficiel : le film Dragon Ball Super : Broly en France le 13 mars (VOSTFR et VF) ! - animeland\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 - bleachmx\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 ? - bleachmx\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l’épisode 7\\nLe site officiel de l’anime promotionnel Super Dragon Ball Heroes: Universal Mission a mis en ligne une affiche, un synopsis ainsi qu’un teaser vidéo pour l’épisode 7. L’épisode 7 sortira le 10 janvier 2019. À voir aussi >>> Dragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records […] ...\\nSource : bleachmx - 30/12/2018 00:02 - trending_up 39\\nSuper Dragon Ball Heroes : Épisode 5, preview date de sortie de l’épisode 6 - bleachmx\\nLe jeu Super Dragon Ball Heroes : World Mission daté en Occident - manga-news\\nDragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records au box office japonais\\nLe journal Mainichi Shimbun a annoncé dans ses pages que le film d’animation Dragon Ball Super: Broly a rapporté 2 milliards de yens (18,1 millions de dollars $) en 11 jours au box office japonais. Il est le film de la franchise a avoir atteint le plus rapidement la barre des 2 milliards de yens. […] ...\\nSource : bleachmx - 26/12/2018 17:46 - trending_up 15\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l’épisode 7 - bleachmx\\nDragon Ball Super – Broly: Un Vegeta enfin respecté, les premières minutes émouvantes et l’avant-première mondiale - bleachmx\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon\\nLe webjournal japonais Mantan Web a révélé que le film animation Dragon Ball Super Broly a généré 2 milliards de yen de recettes (15,9 millions €) en 11 jours ! ...\\nSource : adala-news - 25/12/2018 11:01 - trending_up 43\\nLe film animation Dragon Ball Super Broly, en Trailer 2 - adala-news\\nLe film animation Dragon Ball Super Broly, en Trailer 3 - adala-news\\nTop 5 des films animation Dragon Ball les plus populaires au Japon\\nLa Toei a dévoilé le classement des films animation Dragon Ball préférés des japonais ! ...\\nSource : adala-news - 24/12/2018 00:03 - trending_up 28\\nPremiers chiffres du film animation Dragon Ball Super Broly au Japon - adala-news\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon - adala-news\\nDragon Ball Super Chapitre Scan 043 VF\\nLe chapitre 43 de Dragon Ball Super pour ce mois de décembre 2018 pour clôturer l’année. Le film Dragon Ball Super – Broly est enfin sorti au Japon et le manga entame l’Arc du Prisonnier de la Patrouille Galactique. C’est donc une nouvelle fois le film qui fait la couverture du magazine. Toutes les pages […] ...\\nSource : bleachmx - 21/12/2018 04:00 - trending_up 38\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1548042730000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"https://cultinfos.com/buzz/332814-dragon-ball-20e-film-de-sage-sortira-14-decembre-premiere-image-teaser\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# show the 3 first samples\n",
    "! head -n 1 {multilingual_data} | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d22236-bc9e-4668-ae3c-d35bdbf6f5de",
   "metadata": {},
   "source": [
    "Notice, **languages are not annotated in the dataset**, allowing us to leverage AI-based language separation later in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f276db-4372-4775-b0a0-08b94fe5c3f9",
   "metadata": {},
   "source": [
    "Let's now create a document dataset from a pandas data frame. For more information on the arguments see Dask’s from_pandas documentation\n",
    "\n",
    "NeMo Curator's `DocumentDataset` employs Dask's distributed dataframes to mangage large datasets across multiple nodes and allow for easy restarting of interrupted curation. `DocumentDataset` supports reading and writing to sharded *jsonl* and *parquet* files both on local disk and from remote sources such as S3 bukets.\n",
    "\n",
    "Let's load our multilingual dataset with Nemo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d2d25e-5a7d-4639-9e17-bf09759726c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e230e1fb-898a-47e1-84d0-a2578cd01a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./datasets/multilingual\"\n",
    "multilingual_dataset = DocumentDataset.read_json(\n",
    "    multilingual_data_path, add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b89f475e-0284-4681-aec1-a536e0bd751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1ef92-efd2-434b-96d3-2722bcfcc955",
   "metadata": {},
   "source": [
    "## 1.2 Basic Text cleaning and Unification\n",
    "\n",
    "NeMo Curator provides various `DocumentModifier` implementations such as the `UnicodeReformatter` which uses [ftfy](https://pypi.org/project/ftfy/) (fixes text for you) to resolve all unicode issues in the dataset. \n",
    "\n",
    "It is also possible to implement your custom text cleaner using `DocumentModifier`. For instance, we can standardize inconsistent quotation marks that appear very often in curated large dataset, remove HTML, URLs, and email tags, etc.\n",
    "\n",
    "\n",
    "Let's first create the output folders to save the cleaned step outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4df995-300b-470d-847b-78839ca859b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set dataset file path\n",
    "curated_data_path = \"./curated\"\n",
    "clean_and_unify_data_path = os.path.join(curated_data_path, \"01_clean_and_unify\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(curated_data_path, exist_ok=True)\n",
    "os.makedirs(clean_and_unify_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb4a71-2347-4cd3-a5e9-0711434a467d",
   "metadata": {},
   "source": [
    "Let's now implement a custom text cleaner `QuotationTagUnifier`.\n",
    "\n",
    "It is designed to modify text documents by normalizing quotation marks and removing unwanted elements. \n",
    "\n",
    "The result is a cleaned and standardized text output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dc7a4-d9e7-464d-864b-baa653cab173",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd0a641-4063-4781-8d0d-e5e697ad50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import dask\n",
    "import pandas as pd\n",
    "from nemo_curator.modifiers import DocumentModifier, UnicodeReformatter\n",
    "from nemo_curator.modules.modify import Modify\n",
    "\n",
    "\n",
    "class QuotationTagUnifier(DocumentModifier):\n",
    "    def modify_document(self, text: str) -> str:\n",
    "        text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = re.sub(\n",
    "            r\"(<[^>]+>)|(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\",\n",
    "            \"\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838a49-15d8-4bfd-b5ea-c6c7d09652ba",
   "metadata": {},
   "source": [
    "Next, we can chain modifiers together using the `Sequential` class, which takes a list of operations to be run sequentially and applies them to a given `DocumentDataset`.ipynb_checkpoints/\n",
    " \n",
    "Let's call this sequence the `cleaners`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35cd6bbb-0be0-4089-88d7-e46ca1d21c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import Sequential\n",
    "\n",
    "cleaners = Sequential(\n",
    "    [\n",
    "        # Apply: Unify all the quotation marks and remove tags\n",
    "        Modify(QuotationTagUnifier()),\n",
    "        # Apply: Unify all unicode\n",
    "        Modify(UnicodeReformatter()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43d8e7-a892-46e8-9719-1ea2363d6577",
   "metadata": {},
   "source": [
    "Let's run that on a toy example with few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf51cce-2ff4-4ded-a54d-a12773a8074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play ‘footbal’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is very  \\t  happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visit &lt;a href='www.example.com'&gt;example.com&lt;/a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                    Ryan went out to play ‘footbal’\n",
       "1                             He is very  \\t  happy.\n",
       "2  Visit <a href='www.example.com'>example.com</a..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the toy samples\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play ‘footbal’\",\n",
    "            \"He is very  \\t  happy.\",\n",
    "            \"Visit <a href='www.example.com'>example.com</a> for more information or contact us at info@example.com\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "# check the samples\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705224b3-31d4-4702-97a6-8e2f572b7a6f",
   "metadata": {},
   "source": [
    "Now, let's apply our sequence of cleaners to the toy samples. To execute this sequence on the Dask cluster, we use `.persist()`, which keeps the transformed data in memory for optimized processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc070516-28b6-425d-a9b6-d3792f3ed8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_clean_and_unify = cleaners(dataset_toy).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bc55b-3a28-4825-baa3-cea46491c084",
   "metadata": {},
   "source": [
    "Let's check the output.\n",
    "\n",
    "Expected output are samples with normalized quotations, removed tabs and HTML, URL and Email tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d78c5a8-4548-4106-8f4d-cab9ffc18eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play 'footbal'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is very     happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visit example.com for more information or cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                    Ryan went out to play 'footbal'\n",
       "1                              He is very     happy.\n",
       "2  Visit example.com for more information or cont..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned toy samples\n",
    "dataset_test_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52122f-9a73-421a-a3b7-0b4dd234cb2b",
   "metadata": {},
   "source": [
    "Now, let's apply this cleaning step to our multilingual dataset. We can achieve this by creating a sequence of curation steps, starting with the cleaning sequence as the first function in our data curation pipeline.\n",
    "\n",
    "Run the next cell to create the cleaning step as a function that would be the first curation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e999870-cbe9-4d28-8b02-8d6e6a8f80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sequence of cleaning operations as a function\n",
    "def clean_and_unify(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    cleaners = Sequential(\n",
    "        [\n",
    "            # Apply: Unify all the quotation marks and remove tags\n",
    "            Modify(QuotationTagUnifier()),\n",
    "            # Apply: Unify all unicode\n",
    "            Modify(UnicodeReformatter()),\n",
    "        ]\n",
    "    )\n",
    "    return cleaners(dataset)\n",
    "\n",
    "\n",
    "# sequence of data curation setps. so far, only cclean_and_unify is defined\n",
    "curation_steps = Sequential([clean_and_unify])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af0dae-640c-48ad-9023-25d19a4caa80",
   "metadata": {},
   "source": [
    "Let's now execute this step on out multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29d03f50-ddfa-4cb7-86bf-e06f9bd70c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the pipeline...\n",
      "CPU times: user 15.3 ms, sys: 2.45 ms, total: 17.7 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Executing the pipeline...\")\n",
    "\n",
    "dataset_clean_and_unify = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee639b-99d7-4026-a87a-90c8dd960335",
   "metadata": {},
   "source": [
    "Let's check some outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc175867-7d83-4452-98df-f79bce5d1297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Restaurantes con Web Y Telefono Y Dias Y Horar...</td>\n",
       "      <td>2020-08-11 16:33:05</td>\n",
       "      <td>http://mendoza.guia.clarin.com/restaurantes-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Responsable qualité - Intérim : Emploi et recr...</td>\n",
       "      <td>2020-08-07 01:17:37</td>\n",
       "      <td>https://images3.meteojob.com/Emploi-Interim-Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "3  file.json  Restaurantes con Web Y Telefono Y Dias Y Horar...   \n",
       "4  file.json  Responsable qualité - Intérim : Emploi et recr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  \n",
       "3 2020-08-11 16:33:05  http://mendoza.guia.clarin.com/restaurantes-co...  \n",
       "4 2020-08-07 01:17:37  https://images3.meteojob.com/Emploi-Interim-Re...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2abe75-18db-44eb-b881-be16d046fb47",
   "metadata": {},
   "source": [
    "We can save the created Document into a json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d14fede-e581-4235-81c3-7ace79a593a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save output to json\n",
    "dataset_clean_and_unify.to_json(clean_and_unify_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aea2e91-550d-494a-8794-533531c341c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Dragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser sur Buzz, insolite et culture\\nDragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser\\nLe 20e film Dragon Ball sortira le vendredi 14 décembre 2018. La première affiche teaser montre un Gokû jeune adulte, environ celui de la fin de Dragon Ball et le début de Dragon Ball Z. À lire aussi >>> Le gouvernement mexicain prévoit la diffusion sur place publique des épisodes 130 et 131 de Dragon […]...\\nLire la suite du buzz sur bleachmx Source : bleachmx - 12/03/2018 22:31 - trending_up142\\nfilm commémoration Akira Toriyama dbz dragon ball Dragon Ball Super dragon ball z affiche Dragon Ball Super Anime V Jump Décembre 2018 Dragon Ball Z Battle of Gods Dragon Ball Z Fukkatsu No [F] Dragon Ball Z La Résurrection de [F] Potins Films\\nLe site Deadline indique ce Jeudi que le film d'animation Dragon Ball Super – Broly a rapporté, selon les estimations, plus de 7 millions de dollars pour sa première journée d'exploitation aux États-Unis. Il prévoit que le film rapporte plus de 15 millions de dollars pour sa première semaine. À voir aussi >>> Dragon Ball […] ...\\nSource : bleachmx - 18/01/2019 00:16 - trending_up 15\\nDragon Ball Super Broly: Le film d'animation prévu dans 90 pays - bleachmx\\nDragon Ball Super – Broly: Le film adapté en manga et en light novel - bleachmx\\nLors du DRAGON BALL Games SUPER Showcase il a été annoncé que le je vidéo Super Dragon Ball Heroes: World Mission sortira sur Nintendo Switch et PC (via Steam) le 5 avril 2019 en occident. Le trailer sous-titré a été présenté ainsi qu'une vidéo de gameplays. La vidéo présente Cirrus (Shiirus – Shiirasu), le nouveau […] ...\\nSource : bleachmx - 15/01/2019 01:45 - trending_up 22\\nSuper Dragon Ball Heroes: World Mission : Deuxième trailer du jeu vidéo - bleachmx\\nDes avants-premières pour le film Dragon Ball Super Broly dans les cinémas CGR\\nLe film Dragon Ball Super Broly est attendu comme le messie par les fans de l'univers d'Akira Toriyama. Après les annonces d'avants-premières au ...\\nSource : manga-news - 08/01/2019 11:00 - trending_up 15\\nLes avants-premières françaises du film Dragon Ball Super Broly dévoilées\\nParticulièrement attendu, le film Dragon Ball Super Broly sortira dans les cinémas de France le 13 mars. Mais avant ça, plusieurs avant-premières ...\\nSource : manga-news - 04/01/2019 15:16 - trending_up 16\\nAvant-premi?res de Dragon Ball Super Broly dans toute la France (Les 23 et 24 janvier 2019)\\nLes 23 et 24 janvier 2019 Le film Dragon Ball Super Broly est sorti au Japon le 13 décembre 2018 et paraîtra également au cinéma en France le 13 mars 2019. Avant cette date, plusieurs avant-premières son programmés en janvier. Mercredi 23 et jeudi 24 janvier Le Grand Rex (Paris) Jeudi 24 janvier Pathé : St Herblain, Pathé Nantes-Atlantis (44, Loire-Atlantique) Labège, Gaumont (31, Haute-Garonne) Toulouse, Gaumont Wilson ( 31, Haute-Garonne) Belle-Epine, Pathé (94, Thiais) ... ...\\nSource : animint - 04/01/2019 14:46 - trending_up 26\\nOfficiel : le film Dragon Ball Super : Broly en France le 13 mars (VOSTFR et VF) ! - animeland\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 - bleachmx\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 ? - bleachmx\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l'épisode 7\\nLe site officiel de l'anime promotionnel Super Dragon Ball Heroes: Universal Mission a mis en ligne une affiche, un synopsis ainsi qu'un teaser vidéo pour l'épisode 7. L'épisode 7 sortira le 10 janvier 2019. À voir aussi >>> Dragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records […] ...\\nSource : bleachmx - 30/12/2018 00:02 - trending_up 39\\nSuper Dragon Ball Heroes : Épisode 5, preview date de sortie de l'épisode 6 - bleachmx\\nLe jeu Super Dragon Ball Heroes : World Mission daté en Occident - manga-news\\nDragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records au box office japonais\\nLe journal Mainichi Shimbun a annoncé dans ses pages que le film d'animation Dragon Ball Super: Broly a rapporté 2 milliards de yens (18,1 millions de dollars $) en 11 jours au box office japonais. Il est le film de la franchise a avoir atteint le plus rapidement la barre des 2 milliards de yens. […] ...\\nSource : bleachmx - 26/12/2018 17:46 - trending_up 15\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l'épisode 7 - bleachmx\\nDragon Ball Super – Broly: Un Vegeta enfin respecté, les premières minutes émouvantes et l'avant-première mondiale - bleachmx\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon\\nLe webjournal japonais Mantan Web a révélé que le film animation Dragon Ball Super Broly a généré 2 milliards de yen de recettes (15,9 millions €) en 11 jours ! ...\\nSource : adala-news - 25/12/2018 11:01 - trending_up 43\\nLe film animation Dragon Ball Super Broly, en Trailer 2 - adala-news\\nLe film animation Dragon Ball Super Broly, en Trailer 3 - adala-news\\nTop 5 des films animation Dragon Ball les plus populaires au Japon\\nLa Toei a dévoilé le classement des films animation Dragon Ball préférés des japonais ! ...\\nSource : adala-news - 24/12/2018 00:03 - trending_up 28\\nPremiers chiffres du film animation Dragon Ball Super Broly au Japon - adala-news\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon - adala-news\\nDragon Ball Super Chapitre Scan 043 VF\\nLe chapitre 43 de Dragon Ball Super pour ce mois de décembre 2018 pour clôturer l'année. Le film Dragon Ball Super – Broly est enfin sorti au Japon et le manga entame l'Arc du Prisonnier de la Patrouille Galactique. C'est donc une nouvelle fois le film qui fait la couverture du magazine. Toutes les pages […] ...\\nSource : bleachmx - 21/12/2018 04:00 - trending_up 38\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1548042730000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"https://cultinfos.com/buzz/332814-dragon-ball-20e-film-de-sage-sortira-14-decembre-premiere-image-teaser\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 {clean_and_unify_data_path}/file.jsonl | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89228aa9-641f-4a1d-96b5-61fccb703832",
   "metadata": {},
   "source": [
    "## Dataset document size Filtering\n",
    "\n",
    "Extremely short documents may lack sufficient context or information for the model to learn meaningful concepts. By filtering out such documents, we can ensure that the data used for training is sufficiently informative and balanced.\n",
    "\n",
    "Let's explore how to apply word counts and filtering using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f59e1ac-4a33-4305-abe6-25d3e3ab596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import (\n",
    "    DocumentFilter,\n",
    "    RepeatingTopNGramsFilter,\n",
    "    WordCountFilter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89f424f0-2db3-47b4-a919-666aea19d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncompleteDocumentFilter(DocumentFilter):\n",
    "    \"\"\"\n",
    "    If the document doesn't end with a terminating punctuation mark, then discard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Accepted document terminators.\n",
    "        self._story_terminators = {\".\", \"!\", \"?\", '\"', \"”\"}\n",
    "\n",
    "    def score_document(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a document's score is valid based on the last character of the text.\n",
    "        Args:\n",
    "            text (str): The document text.\n",
    "        Returns:\n",
    "            bool: True if the document's score is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return text.strip()[-1] in self._story_terminators\n",
    "\n",
    "    def keep_document(self, score) -> bool:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402e2b-9b38-4a0d-877b-a62a935f1c89",
   "metadata": {},
   "source": [
    "The following code defines a function, `filter_dataset`, that cleans a `DocumentDataset` by applying several filters:\n",
    "\n",
    "- **Word Count Filter**: Removes documents with fewer than 80 words by default.\n",
    "- **Incomplete Document Filter**: Removes incomplete documents.\n",
    "- **Repeating N-Grams Filters**: Removes documents with excessive repetition of word sequences (2-grams, 3-grams, 4-grams) above certain thresholds (20%, 18%, 16% respectively).\n",
    "\n",
    "These filters are applied sequentially to refine the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c610d1cb-f44f-42fc-bb37-8759a50b1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    filters = Sequential(\n",
    "        [\n",
    "            ScoreFilter(\n",
    "                WordCountFilter(min_words=80),\n",
    "                text_field=\"text\",\n",
    "                score_field=\"word_count\",\n",
    "            ),\n",
    "            ScoreFilter(IncompleteDocumentFilter(), text_field=\"text\"),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=2, max_repeating_ngram_ratio=0.2),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=3, max_repeating_ngram_ratio=0.18),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=4, max_repeating_ngram_ratio=0.16),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return filters(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee9ddd-57b3-4cac-8351-fb8b7fb5acef",
   "metadata": {},
   "source": [
    "Let's now apply that on our multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ef91e32-c9ea-4bf4-b1ab-016ee9413e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the pipeline...\n",
      "CPU times: user 80.4 ms, sys: 8.76 ms, total: 89.2 ms\n",
      "Wall time: 83.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "filtered_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0c2dd-0e63-4105-aeee-a41b12f0fb06",
   "metadata": {},
   "source": [
    "We can check the outputs. Notice that a new field named `word_count` has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72e03f52-5757-41b9-9e09-05c3cb5e67ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Copy-Paste ejecutado en el Windows Phone 7.[Ví...</td>\n",
       "      <td>2019-07-20 07:52:44</td>\n",
       "      <td>https://geeksroom.com/2010/12/copy-paste-ejecu...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Agenda de eventos y actividades en Barcelona p...</td>\n",
       "      <td>2018-07-22 22:13:01</td>\n",
       "      <td>http://barcelona.carpediem.cd/events/?dt=06.04...</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>file.json</td>\n",
       "      <td>EE.UU | EE.UU\\nenero 22, 2014 Juan Pedro Sánch...</td>\n",
       "      <td>2019-09-20 03:50:18</td>\n",
       "      <td>https://makeexperience.wordpress.com/tag/ee-uu/</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>file.json</td>\n",
       "      <td>jandrobell - Pesca Mediterraneo 2\\njandrobell\\...</td>\n",
       "      <td>2018-08-18 11:01:22</td>\n",
       "      <td>http://www.pescamediterraneo2.com/foros/profil...</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name                                               text  \\\n",
       "1   file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "6   file.json  Copy-Paste ejecutado en el Windows Phone 7.[Ví...   \n",
       "8   file.json  Agenda de eventos y actividades en Barcelona p...   \n",
       "11  file.json  EE.UU | EE.UU\\nenero 22, 2014 Juan Pedro Sánch...   \n",
       "13  file.json  jandrobell - Pesca Mediterraneo 2\\njandrobell\\...   \n",
       "\n",
       "             timestamp                                                url  \\\n",
       "1  2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688   \n",
       "6  2019-07-20 07:52:44  https://geeksroom.com/2010/12/copy-paste-ejecu...   \n",
       "8  2018-07-22 22:13:01  http://barcelona.carpediem.cd/events/?dt=06.04...   \n",
       "11 2019-09-20 03:50:18    https://makeexperience.wordpress.com/tag/ee-uu/   \n",
       "13 2018-08-18 11:01:22  http://www.pescamediterraneo2.com/foros/profil...   \n",
       "\n",
       "    word_count  \n",
       "1          111  \n",
       "6          137  \n",
       "8          746  \n",
       "11         323  \n",
       "13        1865  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b41e2-79a7-4a5e-a823-1aa07d29926b",
   "metadata": {},
   "source": [
    "Let's save the output, we need to create the folder first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7840e33-7414-4819-99bc-60e3df51c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_path = os.path.join(curated_data_path, \"02_filter_dataset\")\n",
    "\n",
    "! mkdir -p {filtered_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee80a5b3-8542-42a7-b818-3390653702f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save output to json\n",
    "filtered_dataset.to_json(filtered_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4874-8107-41f0-8358-971011b17fe1",
   "metadata": {},
   "source": [
    "Check the saved file by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f89490af-abab-44ab-919e-9c2847f9384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Cours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition) ➠ Maximilien Samson Frederic Schoell | Буквоед ISBN 978-5-8792-8565-9\\nCours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\nISBN: 978-5-8792-8565-9\\nКод: pod 1702886\\nАвторы: Samson, Schoell\\nС товаром «Cours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)» часто покупают\\nЕсли Вы обнаружили ошибку в описании товара «Cours D'histoire Des États Européens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)» Maximilien Samson Frederic Schoell, выделите её мышкой и нажмите: Ctrl+Enter. Спасибо!\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1547767539000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"https://www.bookvoed.ru/book?id=1433688\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"word_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m111\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 {filtered_data_path}/file.jsonl | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dd4da-9f59-454c-a4c5-725d8cc2a1db",
   "metadata": {},
   "source": [
    "### 1.3 PII Identification and Removal\n",
    "\n",
    "The Personal Identifiable Information (PII) identification tool is designed to remove sensitive data from datasets.\n",
    "\n",
    "The identification leverages [presidio_analyzer](https://pypi.org/project/presidio-analyzer/) a Python based service for detecting PII entities in text.\n",
    "\n",
    "Let's try to analyze a toy sample: *My name is Dana and my number is 212-555-5555*\n",
    "\n",
    "Expected output is the type `PERSON` and `PHONE_NUMBER` and the char start and end position.\n",
    "\n",
    "```\n",
    " type: PERSON, start: 11, end: 15, score: 0.85,\n",
    " type: PHONE_NUMBER, start: 33, end: 45, score: 0.75\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aaef24e-c838-408c-9242-f40cfc11146d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m361.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.12/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (79.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.11.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.3)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "[type: PERSON, start: 11, end: 15, score: 0.85, type: PHONE_NUMBER, start: 33, end: 45, score: 0.75]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "# Hide deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\n",
    "LANGUAGES_CONFIG_FILE = \"./languages-config.yml\"\n",
    "# Create NLP engine based on configuration file\n",
    "provider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "analyzer = AnalyzerEngine(\n",
    "    supported_languages=[\"en\", \"es\", \"fr\"], nlp_engine=nlp_engine_with_spanish\n",
    ")\n",
    "\n",
    "results = analyzer.analyze(\n",
    "    text=\"My name is Dana and my number is 212-555-5555\",\n",
    "    entities=[\"PHONE_NUMBER\", \"PERSON\"],\n",
    "    language=\"en\",\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a163574-b7e2-4100-a054-ec53c0a2ff89",
   "metadata": {},
   "source": [
    "Run the analyzer for French sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "300f2672-9671-4fde-8ac7-270b4c81cbaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMon email est mon@example.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/presidio_analyzer/analyzer_engine.py:192\u001b[39m, in \u001b[36mAnalyzerEngine.analyze\u001b[39m\u001b[34m(self, text, language, entities, correlation_id, score_threshold, return_decision_process, ad_hoc_recognizers, context, allow_list, nlp_artifacts)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# run the nlp pipeline over the given text, store the results in\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# a NlpArtifacts instance\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nlp_artifacts:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     nlp_artifacts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_decision_process:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.app_tracer.trace(\n\u001b[32m    196\u001b[39m         correlation_id, \u001b[33m\"\u001b[39m\u001b[33mnlp artifacts:\u001b[39m\u001b[33m\"\u001b[39m + nlp_artifacts.to_json()\n\u001b[32m    197\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/presidio_analyzer/nlp_engine/spacy_nlp_engine.py:95\u001b[39m, in \u001b[36mSpacyNlpEngine.process_text\u001b[39m\u001b[34m(self, text, language)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nlp:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNLP engine is not loaded. Consider calling .load()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m(text)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._doc_to_nlp_artifact(doc, language)\n",
      "\u001b[31mKeyError\u001b[39m: 'fr'"
     ]
    }
   ],
   "source": [
    "analyzer.analyze(text=\"Mon email est mon@example.com\", language=\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfef904-5323-411f-9b04-7fc8cbf95f81",
   "metadata": {},
   "source": [
    "Try your own examples in these three languages for accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f9d69-1563-48a7-b2ea-4299489c62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = # Your code here\n",
    "analyzer.analyze(text=input, language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa84a63-5062-44bc-b948-f85f246d7e42",
   "metadata": {},
   "source": [
    "Nemo Curator integrates PII Identification and Removal efficiently leveraging Dask for parallelization. The tool currently supports the identification and removal of the following sensitive data types:\n",
    "\n",
    "`ADDRESS`,`CREDIT_CARD`,`EMAIL_ADDRESS`,`DATE_TIME`,`IP_ADDRESS`,`LOCATION`,`PERSON`,`URL`,`US_SSN`,`US_PASSPORT`,`US_DRIVER_LICENSE`,`PHONE_NUMBER`,\n",
    "\n",
    "Let;s run the Nemo Curator PII Identification `PiiModifier` on a toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eff2cc51-7177-4224-a637-5c167c6f7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His email is ryan@example.com and phone is 212...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                     Ryan went out to play football\n",
       "1  His email is ryan@example.com and phone is 212..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create toy samples with PII data\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play football\",\n",
    "            \"His email is ryan@example.com and phone is 212-555-5555\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3f12d-ef3f-48a5-bd1a-1902de2d8c3d",
   "metadata": {},
   "source": [
    "Let's build and apply the `PiiModifier` on the toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1b52e11-bfd9-4aa5-b721-cff7e67ff6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "modifier = PiiModifier(\n",
    "    batch_size=2000,\n",
    "    language=\"en\",\n",
    "    supported_entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],\n",
    "    anonymize_action=\"replace\",\n",
    ")\n",
    "\n",
    "modify = Modify(modifier)\n",
    "modified_dataset = modify(dataset_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "071a86f9-1e93-47e9-a1f8-e88f182bc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 18:04:06 INFO:Loaded recognizer: EmailRecognizer\n",
      "2025-12-07 18:04:06 INFO:Loaded recognizer: PhoneRecognizer\n",
      "2025-12-07 18:04:06 INFO:Loaded recognizer: SpacyRecognizer\n",
      "2025-12-07 18:04:06 INFO:Loaded recognizer: UsSsnRecognizer\n",
      "2025-12-07 18:04:06 INFO:Loaded recognizer: CreditCardRecognizer\n",
      "2025-12-07 18:04:06 INFO:Loaded recognizer: IpRecognizer\n",
      "2025-12-07 18:04:06 WARNING:model_to_presidio_entity_mapping is missing from configuration, using default\n",
      "2025-12-07 18:04:06 WARNING:low_score_entity_names is missing from configuration, using default\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PERSON&gt; went out to play football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His email is &lt;EMAIL_ADDRESS&gt; and phone is &lt;PHO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 <PERSON> went out to play football\n",
       "1  His email is <EMAIL_ADDRESS> and phone is <PHO..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check modified data\n",
    "modified_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1654a77-4330-4902-99bf-056be4920dcb",
   "metadata": {},
   "source": [
    "Now, let's integrate this PII identification step into our curation sequence and apply it to the multilingual dataset. This will ensure that sensitive data is properly detected and removed while maintaining data quality. \n",
    "\n",
    "Let's create first the `redact_pii` function for PII identification and removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e7d0f90-6efa-4ea9-a9e4-c1849e0a637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "\n",
    "def redact_pii(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    redactor = Modify(\n",
    "        PiiModifier(\n",
    "            supported_entities=[\"PERSON\"],\n",
    "            anonymize_action=\"replace\",\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return redactor(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622c883-8fb5-4951-84e7-9f0a80a7ce29",
   "metadata": {},
   "source": [
    "Let's now run the sequence of curation steps including the PII removal function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c0c780b-2632-4ff9-9ddd-7a889bf1f58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the pipeline...\n",
      "CPU times: user 156 ms, sys: 16.5 ms, total: 172 ms\n",
      "Wall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset, redact_pii])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "redact_pii_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4ce773a-9520-44a9-bad4-9915d8a808d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 18:04:19 INFO:Loaded recognizer: EmailRecognizer\n",
      "2025-12-07 18:04:19 INFO:Loaded recognizer: PhoneRecognizer\n",
      "2025-12-07 18:04:19 INFO:Loaded recognizer: SpacyRecognizer\n",
      "2025-12-07 18:04:19 INFO:Loaded recognizer: UsSsnRecognizer\n",
      "2025-12-07 18:04:19 INFO:Loaded recognizer: CreditCardRecognizer\n",
      "2025-12-07 18:04:19 INFO:Loaded recognizer: IpRecognizer\n",
      "2025-12-07 18:04:19 WARNING:model_to_presidio_entity_mapping is missing from configuration, using default\n",
      "2025-12-07 18:04:19 WARNING:low_score_entity_names is missing from configuration, using default\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: &lt;PERSON&gt;...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Copy-Paste ejecutado en el Windows Phone 7.[Ví...</td>\n",
       "      <td>2019-07-20 07:52:44</td>\n",
       "      <td>https://geeksroom.com/2010/12/copy-paste-ejecu...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Agenda de eventos y actividades en Barcelona p...</td>\n",
       "      <td>2018-07-22 22:13:01</td>\n",
       "      <td>http://barcelona.carpediem.cd/events/?dt=06.04...</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>file.json</td>\n",
       "      <td>EE.UU | EE.UU\\nenero 22, 2014 &lt;PERSON&gt;, Met, M...</td>\n",
       "      <td>2019-09-20 03:50:18</td>\n",
       "      <td>https://makeexperience.wordpress.com/tag/ee-uu/</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>file.json</td>\n",
       "      <td>jandrobell - Pesca Mediterraneo 2\\njandrobell\\...</td>\n",
       "      <td>2018-08-18 11:01:22</td>\n",
       "      <td>http://www.pescamediterraneo2.com/foros/profil...</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name                                               text  \\\n",
       "1   file.json  Cours D'histoire Des États Européens: <PERSON>...   \n",
       "6   file.json  Copy-Paste ejecutado en el Windows Phone 7.[Ví...   \n",
       "8   file.json  Agenda de eventos y actividades en Barcelona p...   \n",
       "11  file.json  EE.UU | EE.UU\\nenero 22, 2014 <PERSON>, Met, M...   \n",
       "13  file.json  jandrobell - Pesca Mediterraneo 2\\njandrobell\\...   \n",
       "\n",
       "             timestamp                                                url  \\\n",
       "1  2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688   \n",
       "6  2019-07-20 07:52:44  https://geeksroom.com/2010/12/copy-paste-ejecu...   \n",
       "8  2018-07-22 22:13:01  http://barcelona.carpediem.cd/events/?dt=06.04...   \n",
       "11 2019-09-20 03:50:18    https://makeexperience.wordpress.com/tag/ee-uu/   \n",
       "13 2018-08-18 11:01:22  http://www.pescamediterraneo2.com/foros/profil...   \n",
       "\n",
       "    word_count  \n",
       "1          111  \n",
       "6          137  \n",
       "8          746  \n",
       "11         323  \n",
       "13        1865  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the filtered data\n",
    "redact_pii_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e50d88-8a61-46c0-b42f-94a22a3ad4d3",
   "metadata": {},
   "source": [
    "Let's now save the fileted data. We need to create the folder to save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b610ca1-5d53-4d00-aa78-60c8e019bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "redact_pii_data_path = os.path.join(curated_data_path, \"03_redact_pii_data_path\")\n",
    "\n",
    "! mkdir -p {redact_pii_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8259bb6-64ac-46d7-9813-de0e06ec5005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "redact_pii_dataset.to_json(redact_pii_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dcb0bc4-b310-4bb8-8a3d-a2f3e8806296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Cours D'histoire Des États Européens: <PERSON> L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition) ➠ Maximilien Samson <PERSON> Буквоед ISBN 978-5-8792-8565-9\\nCours D'histoire Des États Européens: <PERSON> L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\nISBN: 978-5-8792-8565-9\\nКод: pod 1702886\\nАвторы: Samson, Schoell\\nС товаром «Cours D'histoire Des États Européens: <PERSON> L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)» часто покупают\\nЕсли Вы обнаружили ошибку в описании товара «Cours D'histoire Des États Européens: <PERSON> L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)» <PERSON>, выделите её мышкой и нажмите: Ctrl+Enter. Спасибо!\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1547767539000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"https://www.bookvoed.ru/book?id=1433688\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"word_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m111\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# check the saved file\n",
    "! head -n 1 {redact_pii_data_path}/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f0d3c-4646-4a24-9e86-de0bc153111d",
   "metadata": {},
   "source": [
    "The current PII removal s Nemo Curator implementation is limited to HPC clusters using Slurm as the resource manager. Check the [documentation](https://github.com/NVIDIA/NeMo-Curator/blob/main/docs/user-guide/personalidentifiableinformationidentificationandremoval.rst) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1f1a1-55e1-407b-a494-c17bb58c9258",
   "metadata": {},
   "source": [
    "---\n",
    "In this notebook, you have used Nemo Curator to apply a sequence of basic text curation steps designed to clean and preprocess the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
