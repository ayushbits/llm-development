{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing High-Quality Hindi Data: Data Curation with NVIDIA NeMo Curator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-source [large language models (LLMs)](https://www.nvidia.com/en-us/glossary/large-language-models/) excel in English but struggle with other languages, especially in South Asia. This is primarily due to a lack of training data in these languages, limited understanding of local cultures, and insufficient tokens to capture unique linguistic structures and expressions. To fully meet customer needs, enterprises in non-English-speaking countries must go beyond generic models and customize them to capture the nuances of their local languages, ensuring a seamless and impactful customer experience.\n",
    "\n",
    "In this tutorial, we will use NeMo Curator to process high-quality Hindi data. We will guide you through the data curation pipeline used and share sample code for each stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- **1. [Prerequisites and Environment setups](#prerequisites-and-environment-setups)**\n",
    "- **2. [Data Collecting](#data-collecting)**\n",
    "- **3. [Data Curation flow](#data-curation-flow)**\n",
    "    - a. [Unicode reformatting](#unicode-reformatting)\n",
    "    - b. [Adding Custom IDs to Documents](#adding-custom-ids-to-documents)\n",
    "    - c. [Exact deduplication](#exact-deduplication)\n",
    "    - d. [Heuristic Quality Filtering](#heuristic-quality-filtering)\n",
    "    - e. [Classifier-based Quality Filtering](#classifier-based-quality-filtering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Environment setups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install NeMo Curator by following the instructions to install the CPU and CUDA-accelerated modules in the README file of the [NeMo Curator repository](https://github.com/NVIDIA/NeMo-Curator/tree/main).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install these additional packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed with data processing, we need to set up a Dask environment. Dask is a flexible, open-source library that enables parallel and distributed computing in Python, allowing us to scale computations across multiple cores or even clusters. By distributing tasks, Dask makes the data handling process significantly faster and more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This notebook was run on a single DGX A100 GPU, with a 128-core CPU and 2TB of RAM to handle the dataset size. Depending on your dataset and computing resources, you may need to adjust the Dask worker configuration below accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35615 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Start a Dask cluster with 12 workers, each limited at 64GB of memory.\n",
    "# You might need to adjust these numbers according to your computing resources.\n",
    "cluster = LocalCluster(n_workers=12, processes=True, memory_limit=\"80GB\")\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collecting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is accessed and downloaded using the Hugging Face Hub. For OSCAR (the Hindi subset dataset, version 23.01, an aggregation of web-crawled data), you need to accept the conditions on the [dataset page](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301) and use a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens) for downloading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download and Convert Datasets to Parquet**\n",
    "\n",
    "The conversion of dataset into Parquet format facilitates efficient handling and processing of large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface in /usr/local/lib/python3.12/dist-packages (0.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `nv` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `nv`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <hf_token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 164/164 [00:01<00:00, 91.35ba/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "672817362"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from datasets import DownloadConfig\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "download_config = DownloadConfig(num_proc=8)\n",
    "# Define paths for raw data\n",
    "raw_data_directory = os.path.join(data_dir, \"raw\")\n",
    "# Load and save Hindi Wikipedia dataset\n",
    "# In this experiment, we'll focus exclusively on the Wikipedia dataset to have a faster runtime and streamline the process.\n",
    "ds = load_hf_dataset(\"wikimedia/wikipedia\", \"20231101.hi\")\n",
    "ds[\"train\"].to_parquet(os.path.join(data_dir, \"wiki_hi_231101.parquet\"))\n",
    "\n",
    "# # Load and save Sangraha Hindi corpus (AI4Bharat's large-scale Hindi dataset)\n",
    "# # This is a high-quality Hindi dataset with 34.5 billion tokens\n",
    "# ds = load_hf_dataset(\"ai4bharat/sangraha\", data_dir=\"verified/hin\", split=\"train[:100000]\")\n",
    "# ds.to_parquet(os.path.join(data_dir, \"sangraha_hindi.parquet\"))\n",
    "\n",
    "# Load and save OSCAR Hindi dataset\n",
    "# ds = load_hf_dataset(\n",
    "#     \"oscar-corpus/OSCAR-2301\",\n",
    "#     language=\"hi\",\n",
    "#     token=True,  # Requires HuggingFace token\n",
    "#     download_config=download_config,\n",
    "#     trust_remote_code=True,\n",
    "#     split=\"train[:50000]\"  # Taking a subset for demo\n",
    "# )\n",
    "# ds.to_parquet(os.path.join(data_dir, \"oscar_hi.parquet\"))\n",
    "\n",
    "# Load and save C4 multilingual Hindi dataset\n",
    "# ds = load_hf_dataset(\n",
    "#     \"allenai/c4\",\n",
    "#     data_files=\"multilingual/c4-hi.*.json.gz\",\n",
    "#     download_config=download_config,\n",
    "#     trust_remote_code=True,\n",
    "#     split=\"train[:30000]\"  # Taking a subset for demo\n",
    "# )\n",
    "# ds.to_parquet(os.path.join(data_dir, \"c4_hi.parquet\"))\n",
    "\n",
    "# Load and save Hindi news dataset from IndicNLP suite\n",
    "# Using a Hindi news corpus for diverse content\n",
    "# ds = load_hf_dataset(\"ai4bharat/IndicNLPNews\", \"hi\", split=\"train[:25000]\")\n",
    "# ds.to_parquet(os.path.join(data_dir, \"hindi_news.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine and Standardize Format**\n",
    "\n",
    "We then combine them into a single dataset, keeping only the \"text\" column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 163093\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already standardized\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "if not os.path.exists(raw_data_directory):\n",
    "    # Combine datasets and standardize format\n",
    "    datasets=[\"datasets/wiki_hi_231101.parquet\"]\n",
    "    # datasets = [\n",
    "    #     os.path.join(data_dir, file)\n",
    "    #     for file in [\"wiki_hi_231101.parquet\", \"c4_hi.parquet\", \"oscar_hi.parquet\", \"sangraha_hindi.parquet\", \"hindi_news.parquet\"]\n",
    "    # ]\n",
    "    \n",
    "    data_files = {\"train\": datasets[0]}\n",
    "    ds = load_hf_dataset(\"parquet\", data_files=data_files)\n",
    "    ds = ds[\"train\"].remove_columns([col for col in ds[\"train\"].column_names if col != \"text\"])\n",
    "    \n",
    "    for d in datasets[1:]:\n",
    "        ds_ = load_hf_dataset(\"parquet\", data_files={\"train\": d})\n",
    "        ds_ = ds_[\"train\"].remove_columns([col for col in ds_[\"train\"].column_names if col != \"text\"])\n",
    "        ds = concatenate_datasets([ds, ds_])\n",
    "else:\n",
    "    print('Data already standardized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shard the Combined Dataset**\n",
    "\n",
    "The combined dataset is then sharded into smaller chunks. Sharding is performed to distribute the data evenly across multiple workers in the Dask cluster, facilitating efficient parallel processing during the data curation stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already sharded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(raw_data_directory):\n",
    "    # Shard the dataset\n",
    "    num_shards = 256\n",
    "    for shard_idx in range(num_shards):\n",
    "        shard = ds.shard(index=shard_idx, num_shards=num_shards)\n",
    "        shard.to_parquet(os.path.join(raw_data_directory, f\"{shard_idx}.parquet\"))\n",
    "else:\n",
    "    print('Data already sharded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Curation flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode reformatting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode reformatting is an essential preprocessing step to ensure that text data is standardized and free of encoding errors, which are common in web-crawled datasets. This is particularly important for Hindi text which uses Devanagari script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nemo_curator import Modify\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.utils.distributed_utils import read_data, write_to_disk\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under\n",
    "\n",
    "# Define paths for Unicode formatted data\n",
    "unicode_formatted_output_path = os.path.join(data_dir, \"formatted\")\n",
    "\n",
    "\n",
    "# Load the raw data\n",
    "def load_dataset(input_data_dir: str, file_type: str = \"parquet\") -> DocumentDataset:\n",
    "    files = list(get_all_files_paths_under(input_data_dir))\n",
    "    # print(files)\n",
    "    raw_data = read_data(files, file_type=file_type, backend=\"pandas\", add_filename=True)\n",
    "    return DocumentDataset(raw_data)\n",
    "\n",
    "if not os.path.exists(unicode_formatted_output_path):\n",
    "    raw_data = load_dataset(raw_data_directory, file_type=\"parquet\")\n",
    "    print(\"Running reformatter now\")\n",
    "    # Initialize the Unicode reformatter\n",
    "    cleaner = Modify(UnicodeReformatter())\n",
    "    \n",
    "    # Apply Unicode reformatting\n",
    "    cleaned_data = cleaner(raw_data)\n",
    "    print(\"Finished reformatting and saving to disk now\")\n",
    "    # Save the cleaned data to disk\n",
    "    write_to_disk(cleaned_data.df, unicode_formatted_output_path, write_to_filename=True, output_type=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Custom IDs to Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with further curation steps, it is advisable to preprocess the dataset by adding a unique ID to each document. These IDs serve as trackers that help in identifying duplicate or low-quality documents throughout the curation process, ensuring that each document remains uniquely identifiable throughout processing. <br>\n",
    "\n",
    "NeMo Curator offers an `AddId` class, which allows users to insert custom IDs into documents using a specified prefix format, such as `<prefix>_<id>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 256 files with blocksize='1gb' / files_per_partition=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/nemo_curator/utils/distributed_utils.py:653: UserWarning: If underlying Parquet data does not have a consistent column order, reading with blocksize might fail. Please use files_per_partition approach instead.\n",
      "  return read_data_blocksize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator import AddId\n",
    "\n",
    "# Define paths for input data and output with added IDs\n",
    "add_id_input_data_dir = unicode_formatted_output_path\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id\")\n",
    "add_id_prefix = \"HI_\"  \n",
    "\n",
    "# Load the formatted dataset\n",
    "dataset = DocumentDataset.read_parquet(add_id_input_data_dir)\n",
    "\n",
    "# Initialize the AddId class with a specified prefix and start index\n",
    "add_id = AddId(id_field=\"id\", id_prefix=add_id_prefix, start_index=0)\n",
    "\n",
    "# Apply the ID addition to the dataset\n",
    "id_dataset = add_id(dataset)\n",
    "# print(id_dataset.df)\n",
    "# Save the dataset with added IDs to disk\n",
    "write_to_disk(id_dataset.df, output_path=added_id_output_path, write_to_filename=False, output_type=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact deduplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact deduplication removes identical duplicates from the dataset. By eliminating exact duplicates, we ensure that each data point contributes uniquely to the training process, enhancing the diversity and overall quality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we'll leverage GPU acceleration by utilizing a Dask CUDA cluster. Since the current cluster is CPU-based, we need to shut it down and start a new one with GPU support.\n",
    "\n",
    "To close the existing cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to initialize the GPU Dask cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 42025 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Spilling is enabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:36139': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "\n",
    "\n",
    "def pre_imports() -> None:\n",
    "    import cudf  # noqa: F401\n",
    "\n",
    "\n",
    "client = get_client(cluster_type=\"gpu\", set_torch_to_use_rmm=False)\n",
    "client.run(pre_imports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is the implementation for exact deduplication:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and directory preparation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.modules import ExactDuplicates\n",
    "\n",
    "# Define input and output paths\n",
    "exact_dedup_input_dataset_dir = added_id_output_path\n",
    "exact_dedup_base_output_path = os.path.join(data_dir, \"exact_dedup\")\n",
    "exact_dedup_log_dir = os.path.join(exact_dedup_base_output_path, \"log\")\n",
    "exact_dedup_output_dir = os.path.join(exact_dedup_base_output_path, \"data\")\n",
    "deduped_output_dir = os.path.join(data_dir, \"remove_duplicate\")\n",
    "\n",
    "# Create directories for logs and output\n",
    "!mkdir -p {exact_dedup_log_dir}\n",
    "!mkdir -p {exact_dedup_output_dir}\n",
    "!mkdir -p {deduped_output_dir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters and load dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/nemo_curator/utils/distributed_utils.py:653: UserWarning: If underlying Parquet data does not have consistent schema, reading with blocksize will fail. Please update underlying RAPIDS package to version 25.02 or higher, or use files_per_partition approach instead.\n",
      "  return read_data_blocksize(\n"
     ]
    }
   ],
   "source": [
    "# Parameters for ExactDuplicates\n",
    "exact_dedup_dataset_id_field = \"id\"\n",
    "exact_dedup_dataset_text_field = \"text\"\n",
    "\n",
    "# Load the input dataset\n",
    "input_dataset = DocumentDataset.read_parquet(exact_dedup_input_dataset_dir, backend=\"cudf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and run deduplication:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/nemo_curator/modules/exact_dedup.py:69: UserWarning: In future NeMo Curator releases, the default value for perform_removal will be True.\n",
      "  super().__init__(\n",
      "/usr/local/lib/python3.12/dist-packages/nemo_curator/modules/base.py:156: UserWarning: Output path f./datasets/exact_dedup/data/_exact_duplicates.parquet already exists and will be overwritten\n",
      "  duplicates = self.identify_duplicates(dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize=None / files_per_partition=1\n",
      "Number of exact duplicate files: 5039\n",
      "Total remaining documents: 163093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/nemo_curator/datasets/doc_dataset.py:440: UserWarning: Consider passing in blocksize for better control over memory usage.\n",
      "  raw_data = read_data(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run exact deduplication\n",
    "exact_dup = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir,\n",
    "    id_field=exact_dedup_dataset_id_field,\n",
    "    text_field=exact_dedup_dataset_text_field,\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_output_dir,\n",
    ")\n",
    "duplicates = exact_dup(dataset=input_dataset)\n",
    "\n",
    "print(f\"Number of exact duplicate files: {len(duplicates)}\")\n",
    "print(f\"Total remaining documents: {len(input_dataset) - len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates and save final dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and exact duplicates to identify and remove duplicate IDs\n",
    "input_dataset = DocumentDataset.read_parquet(added_id_output_path, backend=\"cudf\")\n",
    "exact_duplicates = DocumentDataset.read_parquet(\n",
    "    os.path.join(exact_dedup_output_dir, \"_exact_duplicates.parquet\"),\n",
    "    backend=\"cudf\",\n",
    ")\n",
    "\n",
    "# Extract list of duplicate document IDs\n",
    "exact_docs_to_remove = exact_duplicates.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")],  # noqa: SLF001\n",
    ")\n",
    "\n",
    "# Remove duplicated documents from the input dataset\n",
    "result = input_dataset.df[\n",
    "    ~input_dataset.df[exact_dedup_dataset_id_field].isin(exact_docs_to_remove[exact_dedup_dataset_id_field].compute())\n",
    "]\n",
    "\n",
    "# Save the final deduplicated dataset\n",
    "write_to_disk(result, output_path=deduped_output_dir, write_to_filename=False, output_type=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the GPU Dask cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Quality Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic quality filtering is designed to enhance the quality of the dataset by removing low-quality content based on predefined heuristics. This approach involves applying a series of filters to the dataset to eliminate undesirable data characteristics such as excessive special characters, overly short or long texts, or other criteria that could negatively impact model performance.\n",
    "\n",
    "We use a YAML file to define the heuristic filters. The configuration can be found [here](https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/config/heuristic_filter_non-en.yaml). This file lists the filtering criteria and settings used to build a filter pipeline. You can customize the filters or change thresholds based on your needs for Hindi text processing. The `filter_pipeline` helper reads the YAML settings and applies each filter to the dataset step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate a CPU Dask cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 32993 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Start a Dask cluster with 12 workers, each limited at 64GB of memory.\n",
    "# You might need to adjust these numbers according to your computing resources\n",
    "\n",
    "cluster = LocalCluster(n_workers=12, processes=True, memory_limit=\"64GB\")\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./datasets/remove_duplicate'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./datasets/remove_duplicate...\n",
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n",
      "Loaded 158516 documents\n",
      "\n",
      "Pre-filtering: Removing documents with 0 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/nemo_curator/utils/distributed_utils.py:653: UserWarning: If underlying Parquet data does not have a consistent column order, reading with blocksize might fail. Please use files_per_partition approach instead.\n",
      "  return read_data_blocksize(\n",
      "/usr/local/lib/python3.12/dist-packages/dask/dataframe/dask_expr/_collection.py:4392: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('text', 'int64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing 0-word docs: 155825 documents\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.filters.heuristic_filter import WordCountFilter\n",
    "\n",
    "\n",
    "HF_input_data_dir = deduped_output_dir\n",
    "HF_output_path = os.path.join(data_dir, \"heuristic_filtering\")\n",
    "\n",
    "# Create a directory for the configuration file if it doesn't exist\n",
    "os.makedirs(\"config\", exist_ok=True)\n",
    "# Download the YAML configuration file for heuristic filtering (non-English version suitable for Hindi)\n",
    "# !wget https://raw.githubusercontent.com/NVIDIA-NeMo/Curator/refs/tags/v0.7.0/config/heuristic_filter_non-en.yaml -O ./config/heuristic_filter_non-en.yaml\n",
    "\n",
    "# Specify the path to the configuration file\n",
    "filter_config_file = \"./config/heuristic_filter_non-en.yaml\"\n",
    "os.makedirs(HF_output_path, exist_ok=True)\n",
    "# Load dataset\n",
    "print(f\"Loading dataset from {HF_input_data_dir}...\")\n",
    "dataset = DocumentDataset.read_parquet(\n",
    "    HF_input_data_dir,\n",
    "    backend=\"pandas\",\n",
    "    add_filename=False\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} documents\")\n",
    "\n",
    "# Filter out 0-word documents FIRST, before YAML filters\n",
    "\n",
    "print(\"\\nPre-filtering: Removing documents with 0 words...\")\n",
    "\n",
    "# Use NeMo's own WordCountFilter with min_words=1\n",
    "min_word_filter = ScoreFilter(\n",
    "    WordCountFilter(min_words=10, max_words=1000000000, lang=\"en\"),\n",
    "    text_field=\"text\",\n",
    ")\n",
    "\n",
    "# Apply and PERSIST to force execution NOW\n",
    "dataset = min_word_filter(dataset)\n",
    "\n",
    "print(f\"After removing 0-word docs: {len(dataset)} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 155825 documents\n",
      "After filtering: 3182 documents\n",
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# Load the filters from the YAML configuration file\n",
    "filter_pipeline = build_filter_pipeline(filter_config_file)\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = DocumentDataset.read_parquet(HF_input_data_dir, backend=\"pandas\")\n",
    "print(f\"Loaded {len(dataset)} documents\")\n",
    "\n",
    "# Suppress specific warnings during filtering\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    # Apply the heuristic filters to the dataset\n",
    "    result_data = filter_pipeline(dataset)\n",
    "    print(f\"After filtering: {len(result_data)} documents\")\n",
    "    # Save the filtered dataset to disk\n",
    "    result_data.to_parquet(HF_output_path, write_to_filename=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-based Quality Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier-based filtering uses a trained classifier model to sort content as high or low quality, offering a smarter and more flexible way to handle diverse datasets that simple rules might miss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data for Training Classifier**\n",
    "\n",
    "To train a quality classifier, we need representative samples of both high-quality and low-quality content. For high-quality data, we use articles from Wikipedia's Hindi edition, which are generally well-structured and reliable. The low-quality samples come from unfiltered crawled Hindi news corpus or web data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset as load_hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 58.52ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 158.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 59.78ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 144.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 110.35ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 134.45ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 113.06ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 99.70ba/s] \n"
     ]
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'oscar-corpus/OSCAR-2301' is a gated dataset on the Hub. Visit the dataset page at https://huggingface.co/datasets/oscar-corpus/OSCAR-2301 to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatasetNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m     shard.to_parquet(os.path.join(hq_samples_path, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load and shard the low-quality dataset (Hindi news corpus - using a subset for low quality examples)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Using OSCAR Hindi data which may contain lower quality web-crawled content\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m ds = \u001b[43mload_hf_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moscar-corpus/OSCAR-2301\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain[:100000]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m num_shards = \u001b[32m32\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m shard_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_shards):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/datasets/load.py:2132\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2127\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2128\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2129\u001b[39m )\n\u001b[32m   2131\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2132\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2149\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/datasets/load.py:1853\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1851\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1852\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1853\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1866\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/datasets/load.py:1717\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[32m-> \u001b[39m\u001b[32m1717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/datasets/load.py:1701\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m e.response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m   1700\u001b[39m         message += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Visit the dataset page at https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to ask for access.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1701\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[32m   1704\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRevision \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1705\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mDatasetNotFoundError\u001b[39m: Dataset 'oscar-corpus/OSCAR-2301' is a gated dataset on the Hub. Visit the dataset page at https://huggingface.co/datasets/oscar-corpus/OSCAR-2301 to ask for access."
     ]
    }
   ],
   "source": [
    "# Paths for high-quality and low-quality sample data\n",
    "hq_samples_path = os.path.join(data_dir, \"classifier_filtering/train_samples/hq\")\n",
    "lq_samples_path = os.path.join(data_dir, \"classifier_filtering/train_samples/lq\")\n",
    "\n",
    "# Load and shard the high-quality dataset (Hindi Wikipedia)\n",
    "ds = load_hf_dataset(\"wikimedia/wikipedia\", \"20231101.hi\")\n",
    "num_shards = 8\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = ds[\"train\"].shard(index=shard_idx, num_shards=num_shards)\n",
    "    shard.to_parquet(os.path.join(hq_samples_path, f\"{shard_idx}.parquet\"))\n",
    "\n",
    "# Load and shard the low-quality dataset (Hindi news corpus - using a subset for low quality examples)\n",
    "# Using OSCAR Hindi data which may contain lower quality web-crawled content\n",
    "ds = load_hf_dataset(\n",
    "    \"oscar-corpus/OSCAR-2301\", \n",
    "    language=\"hi\", \n",
    "    token=True,\n",
    "    split=\"train[:100000]\"\n",
    ")\n",
    "num_shards = 32\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = ds.shard(index=shard_idx, num_shards=num_shards)\n",
    "    shard.to_parquet(os.path.join(lq_samples_path, f\"{shard_idx}.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Classifier**\n",
    "\n",
    "The classifier is trained using FastText, which offers an efficient and effective method for text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import Modify\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.utils.distributed_utils import write_to_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from nemo_curator.modifiers import FastTextLabelModifier\n",
    "\n",
    "\n",
    "# Function to create labeled samples\n",
    "def create_samples(data_path: str, label: str, num_samples: int) -> list[str]:\n",
    "    raw_dataset = DocumentDataset.read_parquet(data_path, backend=\"pandas\")\n",
    "    label_quality = Modify(FastTextLabelModifier(label))\n",
    "    labeled_dataset = label_quality(raw_dataset)\n",
    "    labeled_samples = labeled_dataset.df.sample(frac=num_samples / len(labeled_dataset.df))\n",
    "\n",
    "    return labeled_samples[\"text\"].compute().values.tolist()\n",
    "\n",
    "\n",
    "# Prepare training data\n",
    "low_quality_samples = create_samples(lq_samples_path, \"__label__lq\", 100000)\n",
    "high_quality_samples = create_samples(hq_samples_path, \"__label__hq\", 100000)\n",
    "train_samples = low_quality_samples + high_quality_samples\n",
    "random.shuffle(train_samples)\n",
    "\n",
    "# Save training data to a file\n",
    "train_file = \"./cf_model_fasttext_hindi.train\"\n",
    "with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(sample + \"\\n\")\n",
    "\n",
    "# Train the FastText classifier\n",
    "model = fasttext.train_supervised(input=train_file, lr=0.01, dim=100, epoch=5, wordNgrams=2)\n",
    "model_path = \"./cf_model_fasttext_hindi_model.bin\"\n",
    "model.save_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify and Filter the Dataset**\n",
    "\n",
    "Once trained, the classifier is used to filter the dataset, categorizing documents into high and low quality based on the learned distinctions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.filters import FastTextQualityFilter\n",
    "\n",
    "# Define paths and load the dataset\n",
    "CF_input_data_dir = HF_output_path\n",
    "CF_output_path = os.path.join(data_dir, \"classifier_filtering/output\")\n",
    "target_dataset = DocumentDataset.read_parquet(CF_input_data_dir, \"parquet\")\n",
    "\n",
    "# Set up the filtering pipeline\n",
    "filter_pipeline = ScoreFilter(FastTextQualityFilter(model_path), score_field=\"quality_score\", score_type=float)\n",
    "filtered_dataset = filter_pipeline(target_dataset)\n",
    "\n",
    "# Save the filtered dataset\n",
    "write_to_disk(filtered_dataset.df, output_path=CF_output_path, write_to_filename=True, output_type=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the CPU Dask cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how to process and curate high-quality Hindi data using NVIDIA NeMo Curator.\n",
    "\n",
    "- **Datasets**: \n",
    "  - Hindi Wikipedia (wikimedia/wikipedia, \"20231101.hi\")\n",
    "  - Sangraha Hindi corpus (ai4bharat/sangraha)\n",
    "  - OSCAR Hindi dataset\n",
    "  - C4 multilingual Hindi\n",
    "  - Hindi news from IndicNLP suite\n",
    "- **Model Names**: Updated FastText model names to include \"hindi\"\n",
    "\n",
    "**Complete Pipeline Steps Covered:**\n",
    "- ✅ Data collection from multiple Hindi sources\n",
    "- ✅ Unicode reformatting (important for Devanagari script)\n",
    "- ✅ Document ID assignment\n",
    "- ✅ Exact deduplication (GPU-accelerated)\n",
    "- ✅ Heuristic quality filtering\n",
    "- ✅ Classifier-based quality filtering\n",
    "\n",
    "**Hindi-Specific Datasets Used:**\n",
    "1. **Hindi Wikipedia**: High-quality encyclopedic content\n",
    "2. **Sangraha Corpus**: AI4Bharat's large-scale cleaned Hindi dataset (34.5B tokens)\n",
    "3. **OSCAR Hindi**: Web-crawled Hindi content\n",
    "4. **C4 Multilingual Hindi**: Cleaned Common Crawl data\n",
    "5. **IndicNLP News**: Hindi news articles for diversity\n",
    "\n",
    "**What This Notebook Provides:**\n",
    "- Complete end-to-end Hindi data curation pipeline\n",
    "- GPU-accelerated deduplication using cuDF\n",
    "- Quality assessment using both heuristic and ML-based approaches\n",
    "- Hindi-specific considerations for Devanagari script processing\n",
    "- Ready-to-use configuration for Hindi language processing\n",
    "\n",
    "For other techniques such as Fuzzy Deduplication or PII redaction, you can go to [NeMo Curator example scripts](https://github.com/NVIDIA/NeMo-Curator/tree/main/examples).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
