{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81f1c75-fe21-4823-ace7-2ec4a93de023",
   "metadata": {},
   "source": [
    "# 2. Advanced Data Processing\n",
    "\n",
    "\n",
    "In this notebook, we will use NeMo Curator to perform several crutial data cleaning steps, such as language detection and filtering, topic classification, and deduplication. \n",
    "\n",
    "This notebook is structured as follows:\n",
    "- First, we will explore language detection and filtering to separate our multilingual dataset by language.\n",
    "- Next, we will dive into topic classification to categorize the datasets into relevant themes.\n",
    "- Finally, we will explore document deduplication, covering both exact and fuzzy methods.\n",
    "\n",
    "\n",
    "**[2.1 Language Separation](#2.1-Language-Separation)<br>**\n",
    "**[2.2 Domain Classification](#2.2-Domain-Classification)<br>**\n",
    "**[2.3 Documents Deduplication](#2.3-Deduplication)<br>**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bdf4c0-2133-441e-b12c-f28ad3aa3b37",
   "metadata": {},
   "source": [
    "***************\n",
    "### Environment Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70757a02-0847-4100-9dbd-10dbf90d2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore any warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cadc6-7228-45fd-b7d7-bf6c165f35d2",
   "metadata": {},
   "source": [
    "The next cell starts a Dask LocalCluster on your GPU cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067edcdf-34f3-476f-8309-a2726be88c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Spilling is enabled\n",
      "Number of dask worker:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:42049': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client, get_num_workers\n",
    "\n",
    "\n",
    "def pre_imports():\n",
    "    import cudf\n",
    "\n",
    "\n",
    "client = get_client(cluster_type=\"gpu\", set_torch_to_use_rmm=False)\n",
    "\n",
    "print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d38a4-c4bf-4819-a3e4-e85dc96e0884",
   "metadata": {},
   "source": [
    "Let's load the multilingual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b611fce2-537e-4a89-8fb5-c6624b774528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./datasets/multilingual\"\n",
    "multilingual_dataset = DocumentDataset.read_json(\n",
    "    multilingual_data_path, add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518de0e9-9385-4acb-a497-4e5f16c97813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Restaurantes con Web Y Telefono Y Dias Y Horar...</td>\n",
       "      <td>2020-08-11 16:33:05</td>\n",
       "      <td>http://mendoza.guia.clarin.com/restaurantes-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Responsable qualité - Intérim : Emploi et recr...</td>\n",
       "      <td>2020-08-07 01:17:37</td>\n",
       "      <td>https://images3.meteojob.com/Emploi-Interim-Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "3  file.json  Restaurantes con Web Y Telefono Y Dias Y Horar...   \n",
       "4  file.json  Responsable qualité - Intérim : Emploi et recr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  \n",
       "3 2020-08-11 16:33:05  http://mendoza.guia.clarin.com/restaurantes-co...  \n",
       "4 2020-08-07 01:17:37  https://images3.meteojob.com/Emploi-Interim-Re...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data\n",
    "multilingual_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37184b1-73f6-48b4-a41a-e88b662e332e",
   "metadata": {},
   "source": [
    "## 2.1 Language Separation\n",
    "\n",
    "In this section, we will use a language classification model by [fasttext](https://fasttext.cc/docs/en/language-identification.html). \n",
    "\n",
    "\n",
    "Let's first create the output folders and download the fasttext model for text language detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b982ba8-0d1e-424c-9d38-446eef55d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "language_base_output_path = \"./datasets/multilingual/language_separation\"\n",
    "language_separated_output_path = os.path.join(language_base_output_path, \"language\")\n",
    "\n",
    "# Create directories (with parents as needed)\n",
    "os.makedirs(language_base_output_path, exist_ok=True)\n",
    "os.makedirs(language_separated_output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b5f42b-752e-492c-908e-38738a970f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./datasets/multilingual/language_separation/language'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_separated_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fdc0b-32b1-4ee4-bfb2-239cbb458dff",
   "metadata": {},
   "source": [
    "Let's create the filter which uses the downloaded fasttext model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5599b5-9e9b-4d5a-b862-932a4f5bd3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-07 18:06:50--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 52.84.217.124, 52.84.217.55, 52.84.217.5, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|52.84.217.124|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 131266198 (125M) [application/octet-stream]\n",
      "Saving to: ‘./datasets/multilingual/language_separation/language/lid.176.bin’\n",
      "\n",
      "lid.176.bin         100%[===================>] 125.18M   382MB/s    in 0.3s    \n",
      "\n",
      "2025-12-07 18:06:50 (382 MB/s) - ‘./datasets/multilingual/language_separation/language/lid.176.bin’ saved [131266198/131266198]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download fasttext language classification model(this needs to be done hidden in the env)\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -P {language_separated_output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f093c3cc-c109-473d-bc6a-70070f568edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.filters import FastTextLangId\n",
    "\n",
    "lang_filter = FastTextLangId(\"datasets/multilingual/language_separation/language/lid.176.bin\")\n",
    "language_field = \"language\"\n",
    "language_id_pipeline = ScoreFilter(\n",
    "    lang_filter, score_field=language_field, score_type=\"object\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623a093-35aa-47f0-9fb0-a4326ce57cd2",
   "metadata": {},
   "source": [
    "Now, let's apply the language detection filter on our multilingual dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a7f41b5-b5c5-4077-8898-5d989238cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply language separation to our multilingual dataset\n",
    "filtered_dataset = language_id_pipeline(multilingual_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541db02c-5f57-4ef1-9414-fc7a8473ee7f",
   "metadata": {},
   "source": [
    "Let's check the detected language for each sample. \n",
    "\n",
    "Notice the new fields `language` in the output with the language code `FR/EN/ES`and the classification score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4b39506-eb40-44aa-a337-a135042c401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "      <td>[0.9175292253494263, FR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "      <td>[0.5166642069816589, FR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "      <td>[0.9740189909934998, ES]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "\n",
       "            timestamp                                                url  \\\n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...   \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688   \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...   \n",
       "\n",
       "                   language  \n",
       "0  [0.9175292253494263, FR]  \n",
       "1  [0.5166642069816589, FR]  \n",
       "2  [0.9740189909934998, ES]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the detected language per item\n",
    "filtered_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07606f48-a5d3-45f1-a6e4-19e9792b2977",
   "metadata": {},
   "source": [
    "Let's separate documents by the language label and save each language separately. This will create sub-folders for each languages under the output path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62a96f78-6563-47d9-84c9-1c53d50ad70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save separated languages and get stats\n",
    "from nemo_curator.utils.file_utils import separate_by_metadata\n",
    "\n",
    "filtered_dataset.df[language_field] = filtered_dataset.df[language_field].apply(\n",
    "    lambda score: score[1], meta=(language_field, \"object\")\n",
    ")\n",
    "language_stats = separate_by_metadata(\n",
    "    filtered_dataset.df, language_separated_output_path, metadata_field=language_field\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7abd47e-990d-4e8c-a828-d0dee12920cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document:400\n",
      "Number of filtered document:396\n",
      "Language separation stats and   {'FR': 194, 'ES': 194, 'EN': 8}\n"
     ]
    }
   ],
   "source": [
    "# check the language distribution stats\n",
    "print(f\"Number of document:{len(multilingual_dataset)}\")\n",
    "print(f\"Number of filtered document:{len(filtered_dataset)}\")\n",
    "\n",
    "print(\"Language separation stats and  \", language_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d8423-d974-4b25-8778-8231b4ccf8e8",
   "metadata": {},
   "source": [
    "We can check the output jsonl file per language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e521686-a780-4218-a32c-272fc1c3b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Dragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser sur Buzz, insolite et culture\\nDragon Ball: Le 20e film de la sage sortira le 14 décembre, première image teaser\\nLe 20e film Dragon Ball sortira le vendredi 14 décembre 2018. La première affiche teaser montre un Gokû jeune adulte, environ celui de la fin de Dragon Ball et le début de Dragon Ball Z. À lire aussi >>> Le gouvernement mexicain prévoit la diffusion sur place publique des épisodes 130 et 131 de Dragon […]...\\nLire la suite du buzz sur bleachmx Source : bleachmx - 12/03/2018 22:31 - trending_up142\\nfilm commémoration Akira Toriyama dbz dragon ball Dragon Ball Super dragon ball z affiche Dragon Ball Super Anime V Jump Décembre 2018 Dragon Ball Z Battle of Gods Dragon Ball Z Fukkatsu No [F] Dragon Ball Z La Résurrection de [F] Potins Films\\nLe site Deadline indique ce Jeudi que le film d’animation Dragon Ball Super – Broly a rapporté, selon les estimations, plus de 7 millions de dollars pour sa première journée d’exploitation aux États-Unis. Il prévoit que le film rapporte plus de 15 millions de dollars pour sa première semaine. À voir aussi >>> Dragon Ball […] ...\\nSource : bleachmx - 18/01/2019 00:16 - trending_up 15\\nDragon Ball Super Broly: Le film d’animation prévu dans 90 pays - bleachmx\\nDragon Ball Super – Broly: Le film adapté en manga et en light novel - bleachmx\\nLors du DRAGON BALL Games SUPER Showcase il a été annoncé que le je vidéo Super Dragon Ball Heroes: World Mission sortira sur Nintendo Switch et PC (via Steam) le 5 avril 2019 en occident. Le trailer sous-titré a été présenté ainsi qu’une vidéo de gameplays. La vidéo présente Cirrus (Shiirus – Shiirasu), le nouveau […] ...\\nSource : bleachmx - 15/01/2019 01:45 - trending_up 22\\nSuper Dragon Ball Heroes: World Mission : Deuxième trailer du jeu vidéo - bleachmx\\nDes avants-premières pour le film Dragon Ball Super Broly dans les cinémas CGR\\nLe film Dragon Ball Super Broly est attendu comme le messie par les fans de l'univers d'Akira Toriyama. Après les annonces d'avants-premières au ...\\nSource : manga-news - 08/01/2019 11:00 - trending_up 15\\nLes avants-premières françaises du film Dragon Ball Super Broly dévoilées\\nParticulièrement attendu, le film Dragon Ball Super Broly sortira dans les cinémas de France le 13 mars. Mais avant ça, plusieurs avant-premières ...\\nSource : manga-news - 04/01/2019 15:16 - trending_up 16\\nAvant-premi?res de Dragon Ball Super Broly dans toute la France (Les 23 et 24 janvier 2019)\\nLes 23 et 24 janvier 2019 Le film Dragon Ball Super Broly est sorti au Japon le 13 décembre 2018 et paraîtra également au cinéma en France le 13 mars 2019. Avant cette date, plusieurs avant-premières son programmés en janvier. Mercredi 23 et jeudi 24 janvier Le Grand Rex (Paris) Jeudi 24 janvier Pathé : St Herblain, Pathé Nantes-Atlantis (44, Loire-Atlantique) Labège, Gaumont (31, Haute-Garonne) Toulouse, Gaumont Wilson ( 31, Haute-Garonne) Belle-Epine, Pathé (94, Thiais) ... ...\\nSource : animint - 04/01/2019 14:46 - trending_up 26\\nOfficiel : le film Dragon Ball Super : Broly en France le 13 mars (VOSTFR et VF) ! - animeland\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 - bleachmx\\nDragon Ball Super – Broly : Le film dans les cinémas français en XXXX 2019 ? - bleachmx\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l’épisode 7\\nLe site officiel de l’anime promotionnel Super Dragon Ball Heroes: Universal Mission a mis en ligne une affiche, un synopsis ainsi qu’un teaser vidéo pour l’épisode 7. L’épisode 7 sortira le 10 janvier 2019. À voir aussi >>> Dragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records […] ...\\nSource : bleachmx - 30/12/2018 00:02 - trending_up 39\\nSuper Dragon Ball Heroes : Épisode 5, preview date de sortie de l’épisode 6 - bleachmx\\nLe jeu Super Dragon Ball Heroes : World Mission daté en Occident - manga-news\\nDragon Ball Super – Broly : 2 milliards de yens de recettes, le film bat des records au box office japonais\\nLe journal Mainichi Shimbun a annoncé dans ses pages que le film d’animation Dragon Ball Super: Broly a rapporté 2 milliards de yens (18,1 millions de dollars $) en 11 jours au box office japonais. Il est le film de la franchise a avoir atteint le plus rapidement la barre des 2 milliards de yens. […] ...\\nSource : bleachmx - 26/12/2018 17:46 - trending_up 15\\nSuper Dragon Ball Heroes : Épisode 6, preview et date de sortie de l’épisode 7 - bleachmx\\nDragon Ball Super – Broly: Un Vegeta enfin respecté, les premières minutes émouvantes et l’avant-première mondiale - bleachmx\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon\\nLe webjournal japonais Mantan Web a révélé que le film animation Dragon Ball Super Broly a généré 2 milliards de yen de recettes (15,9 millions €) en 11 jours ! ...\\nSource : adala-news - 25/12/2018 11:01 - trending_up 43\\nLe film animation Dragon Ball Super Broly, en Trailer 2 - adala-news\\nLe film animation Dragon Ball Super Broly, en Trailer 3 - adala-news\\nTop 5 des films animation Dragon Ball les plus populaires au Japon\\nLa Toei a dévoilé le classement des films animation Dragon Ball préférés des japonais ! ...\\nSource : adala-news - 24/12/2018 00:03 - trending_up 28\\nPremiers chiffres du film animation Dragon Ball Super Broly au Japon - adala-news\\nLe film animation Dragon Ball Super Broly rapporte 15 millions € en 11 jours au Japon - adala-news\\nDragon Ball Super Chapitre Scan 043 VF\\nLe chapitre 43 de Dragon Ball Super pour ce mois de décembre 2018 pour clôturer l’année. Le film Dragon Ball Super – Broly est enfin sorti au Japon et le manga entame l’Arc du Prisonnier de la Patrouille Galactique. C’est donc une nouvelle fois le film qui fait la couverture du magazine. Toutes les pages […] ...\\nSource : bleachmx - 21/12/2018 04:00 - trending_up 38\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1548042730000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"https://cultinfos.com/buzz/332814-dragon-ball-20e-film-de-sage-sortira-14-decembre-premiere-image-teaser\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"language\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"FR\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# check first element for French\n",
    "! head -n 1 {language_separated_output_path}/FR/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa068ada-9e72-4d4d-91ee-8fb7cc44d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Se realizó una jornada de promoción del buentrato hacia los adultos mayores en Río Gallegos - Ministerio de Desarrollo Social\\nSe realizó una jornada de promoción del buentrato hacia los adultos mayores en Río Gallegos\\nFue el fin de semana pasado en la capital santacruceña. El sábado tuvo lugar una capacitación sobre deporte social. El domingo más de 350 personas participaron de actividades recreativas, charlas y talleres.\\nSe realizó la jornada de promoción del buentrato hacia los adultos mayores en Río Gallegos.\\nFue el fin de semana pasado en el SUM de Vialidad Nacional de la capital santacruceña.\\nEl sábado tuvo lugar una capacitación sobre deporte social.\\nEl domingo más de 350 personas participaron de actividades recreativas, charlas y talleres.\\nEl Ministerio de Desarrollo Social a través del Centro de Referencia de Santa Cruz (CDR) llevó a cabo una jornada de promoción del buentrato hacia los adultos mayores en la ciudad de Río Gallegos. Las actividades se desarrollaron durante el fin de semana pasado en el marco del “Día mundial contra el abuso y el maltrato a los mayores”.\\nEl sábado, desde el proyecto “Madurar en Positivo” perteneciente al Plan Nacional de Deporte Social, se realizó una capacitación en el salón de usos múltiples de Vialidad Nacional dirigida a más de 50 ciudadanos. Asimismo el domingo, más de 350 adultos mayores participaron de actividades recreativas y disfrutaron de juegos de kermés, tejo, sapo, bingo, ping pong, vóleibol adaptado, bowling y talleres.\\nLa Subsecretaría de Responsabilidad Social brindó una charla para concientizar acerca de la importancia de reducir los factores de riesgo asociados a la vida sedentaria, promoviendo hábitos saludables para mejorar la calidad de vida. Por su parte, la Dirección de Deporte Social llevó a cabo el “taller de la risa, donde a través de técnicas de juegos la directora Nacional, Patricia Borrillo habló de las herramientas para que las personas mayores aprendan a distanciarse de las preocupaciones y cuenten con distintas alternativas ante la resolución de un problema.\\nA su vez, los presentes pudieron acceder de manera gratuita, a una unidad móvil sanitaria del programa “Argentina Sonríe” del Ministerio de Salud de Nación. También se hicieron controles de glucemia, presión y vacunación. Por la tarde, se presentaron diversos números artísticos para compartir en familia: el grupo Papelnonos, el coro del Centro de Jubilados “El Despertar” y el grupo de danzas del Centro de Jubilados “La Amistad” y del centro “Encuentro de Amigos”.\\nAl finalizar la jornada, el referente del CDR local, Ariel Fernández, destacó las acciones conjuntas entre todos los organismos presentes, afirmando que la actividad no tiene que ver con el trabajo de un ministerio sino con “un proyecto político con una mirada integral”.\\nLa cartera social a través de los CDR y en articulación con todos los organismos del Gobierno nacional, concibe a las personas mayores como protagonistas del cambio social y promotores de la cultura del buentrato. De esta manera, se trabajan acciones diarias en pos de mejorar la calidad de vida de los adultos mayores.\\n\\\"Sean transgresores, no le digan que sí a todo. Acuérdense la experiencia no se jubila. El desafío es que todos los adultos mayores sean sujetos activos de derecho\\\".\\nLos mayores enseñan a los chicos los juegos de su infancia\\nEl Hogar Balestra cumplió 90 años\\nConocé más sobre Adultos mayores\\nEncontrá dónde consultar sobre Adultos mayores\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1524296308000\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"http://www.desarrollosocial.gob.ar/noticias/se-realizo-una-jornada-de-promocion-del-buentrato-hacia-los-adultos-mayores-en-rio-gallegos/\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"language\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ES\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# check first element for spanish\n",
    "! head -n 1 {language_separated_output_path}/ES/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ca859-98b8-4de9-aa6e-0fa1808feb06",
   "metadata": {},
   "source": [
    "## 2.2 Domain Classification\n",
    "\n",
    "Nemo Curator supports various text classification models allowing data annotation, useful for cleaning and data blending. Check the documentation for [distributed data classification](https://github.com/NVIDIA/NeMo-Curator/blob/main/tutorials/distributed_data_classification/README.md).\n",
    "\n",
    "\n",
    "Each classifier is available on Hugging Face Hub. When run with NeMo Curator, they are accelerated using RAPIDS [CrossFit](https://github.com/rapidsai/crossfit) library.\n",
    "\n",
    "\n",
    "In this section, we will experiment with the `MultilingualDomainClassifier` a Multilingual Domain Classifier that support 52 languages and annotate 26 domain classes:\n",
    "\n",
    "`Arts_and_Entertainment`, `Autos_and_Vehicles`, `Adult`,`Beauty_and_Fitness`, `Books_and_Literature`, `Business_and_Industrial`, `Computers_and_Electronics`, `Finance`, `Food_and_Drink`, `Games`, `Health`, `Hobbies_and_Leisure`, `Home_and_Garden`, `Internet_and_Telecom`, `Jobs_and_Education`, `Law_and_Government`, `News`, `Online_Communities`, `People_and_Society`, `Pets_and_Animals`, `Real_Estate`, `Science`, `Sensitive_Subjects`, `Shopping`, `Sports`, `Travel_and_Transportation`\n",
    "\n",
    "The model architecture is a transformer-based encoder Deberta V3 Base available on Hugging Face Hub. Learn more about the classifier [MultilingualDomainClassifier Model's Card](https://huggingface.co/nvidia/multilingual-domain-classifier).\n",
    "\n",
    "\n",
    "Let's set the output folder for domain classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18d423c-9f35-4357-927d-6fab48b62547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import dask_cudf\n",
    "from nemo_curator.classifiers import MultilingualDomainClassifier\n",
    "\n",
    "domain_output_path = \"./datasets/multilingual/domain_classification\"\n",
    "\n",
    "# Create directory (with parents if needed)\n",
    "os.makedirs(domain_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd61cd-8451-43e2-a4a3-612bc040d70e",
   "metadata": {},
   "source": [
    "First, let's apply the Multilingual Domain Classifier on a toy multilingual dataset. Let's create the dataset with multiple languages and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4639764f-6854-4614-a7a4-95451875622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "text = [\n",
    "    # French\n",
    "    \"Il adore les chats.\",\n",
    "    # English\n",
    "    \"Investing in index funds is a popular strategy for long-term financial growth.\",\n",
    "    # Spanish\n",
    "    \"Ir de compras en el centro comercial es una excelente manera de encontrar ofertas y descubrir nuevas tiendas.\",\n",
    "    # Polish\n",
    "    \"Dzięki wykorzystaniu analizy danych programy treningowe dla sportowców stały się bardziej wyrafinowane.\",\n",
    "    # Arabic\n",
    "    \".تقدم التطورات الحديثة في العلاج الجيني أملاً جديدًا لعلاج الاضطرابات الوراثية\",\n",
    "]\n",
    "df = cudf.DataFrame({\"text\": text})\n",
    "\n",
    "toy_dataset = DocumentDataset(dask_cudf.from_cudf(df, npartitions=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07561f-9b22-4af9-813b-040d548ef251",
   "metadata": {},
   "source": [
    "We can define the `MultilingualDomainClassifier` filter as follows. \n",
    "\n",
    "On its first run, it will download the DeBERTa model from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eae8db38-7b1f-49ce-aea6-92dcdd1d51d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0a0+5228986c39.nv25.06 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting memory estimate curve for model: microsoft/mdeberta-v3-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:01<00:03,  1.99it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:01<00:02,  2.12it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:02<00:02,  1.95it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:02<00:01,  2.26it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:03<00:00,  2.65it/s]\u001b[A\n",
      "Sequence length: 100%|██████████| 8/8 [00:03<00:00,  2.79it/s]\u001b[A\n",
      "Batch size:  12%|█▎        | 1/8 [00:03<00:23,  3.35s/it]     \u001b[A\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:02,  3.29it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:00<00:01,  3.20it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:00<00:01,  2.99it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:01<00:01,  2.99it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:02<00:00,  2.32it/s]\u001b[A\n",
      "Sequence length: 100%|██████████| 8/8 [00:02<00:00,  2.42it/s]\u001b[A\n",
      "Batch size:  25%|██▌       | 2/8 [00:06<00:18,  3.14s/it]     \u001b[A\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:02,  3.12it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:00<00:01,  3.17it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:01<00:01,  2.86it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:02<00:00,  2.73it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:02<00:00,  2.31it/s]\u001b[A\n",
      "Sequence length: 100%|██████████| 8/8 [00:03<00:00,  1.88it/s]\u001b[A\n",
      "Batch size:  38%|███▊      | 3/8 [00:09<00:16,  3.27s/it]     \u001b[A\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:02,  2.86it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:00<00:02,  2.98it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:00<00:01,  3.04it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:01<00:01,  2.67it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:02<00:00,  2.17it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:03<00:00,  1.68it/s]\u001b[A\n",
      "Sequence length: 100%|██████████| 8/8 [00:04<00:00,  1.31it/s]\u001b[A\n",
      "Batch size:  50%|█████     | 4/8 [00:14<00:14,  3.72s/it]     \u001b[A\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:02,  2.90it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:00<00:01,  3.01it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:01<00:01,  2.26it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:02<00:01,  1.72it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:04<00:00,  1.27it/s]\u001b[A\n",
      "Sequence length: 100%|██████████| 8/8 [00:05<00:00,  1.06s/it]\u001b[A\n",
      "Batch size:  62%|██████▎   | 5/8 [00:19<00:13,  4.43s/it]     \u001b[A\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:02,  2.84it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:01<00:01,  2.54it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:02<00:01,  1.91it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:03<00:01,  1.41it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:04<00:00,  1.04it/s]\u001b[A\n",
      "Sequence length: 100%|██████████| 8/8 [00:07<00:00,  1.37s/it]\u001b[A\n",
      "Batch size:  75%|███████▌  | 6/8 [00:26<00:10,  5.32s/it]     \u001b[A\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:02,  2.82it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:01<00:01,  2.89it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:01<00:01,  2.30it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:02<00:01,  1.64it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:03<00:01,  1.20it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:05<00:01,  1.20s/it]\u001b[A\n",
      "Batch size:  88%|████████▊ | 7/8 [00:34<00:06,  6.21s/it]     \u001b[A\n",
      "Sequence length:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length:  12%|█▎        | 1/8 [00:00<00:02,  2.96it/s]\u001b[A\n",
      "Sequence length:  25%|██▌       | 2/8 [00:00<00:02,  2.99it/s]\u001b[A\n",
      "Sequence length:  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]\u001b[A\n",
      "Sequence length:  50%|█████     | 4/8 [00:01<00:01,  2.06it/s]\u001b[A\n",
      "Sequence length:  62%|██████▎   | 5/8 [00:02<00:02,  1.44it/s]\u001b[A\n",
      "Sequence length:  75%|███████▌  | 6/8 [00:04<00:01,  1.02it/s]\u001b[A\n",
      "Sequence length:  88%|████████▊ | 7/8 [00:06<00:01,  1.45s/it]\u001b[A\n",
      "Batch size: 100%|██████████| 8/8 [00:43<00:00,  5.44s/it]     \u001b[A\n"
     ]
    }
   ],
   "source": [
    "# create the classifier\n",
    "domain_classifier = MultilingualDomainClassifier(batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9b726-51e1-4da0-89ad-075b47353aca",
   "metadata": {},
   "source": [
    "Now, let's run the filter on our multilingual multi topics toy samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebac187d-02c7-4f04-8c1e-6edb5b7a85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multilingual domain classifier inference\n",
      "CPU times: user 439 ms, sys: 14.6 ms, total: 453 ms\n",
      "Wall time: 727 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_domain = domain_classifier(dataset=toy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a65249-94f4-4783-826a-50438521cecb",
   "metadata": {},
   "source": [
    "Check the outputs. Notice the new field `domain_pred`. Example of expected outputs: \n",
    "```\n",
    "Il adore les chats.\t                                Pets_and_Animals\n",
    "Investing in index funds is a popular strategy...\tFinance\n",
    "Ir de compras en el centro comercial es una ex...\tShopping\n",
    "Dzięki wykorzystaniu analizy danych programy t...\tSports\n",
    ".تقدم التطورات الحديثة في العلاج الجيني أملاً ...\t        Health\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a42cf0fe-ca35-458a-81fa-53d548bb3a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU: tcp://127.0.0.1:42049, Part: 0: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>domain_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Il adore les chats.</td>\n",
       "      <td>Pets_and_Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investing in index funds is a popular strategy...</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ir de compras en el centro comercial es una ex...</td>\n",
       "      <td>Shopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dzięki wykorzystaniu analizy danych programy t...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.تقدم التطورات الحديثة في العلاج الجيني أملاً ...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       domain_pred\n",
       "0                                Il adore les chats.  Pets_and_Animals\n",
       "1  Investing in index funds is a popular strategy...           Finance\n",
       "2  Ir de compras en el centro comercial es una ex...          Shopping\n",
       "3  Dzięki wykorzystaniu analizy danych programy t...            Sports\n",
       "4  .تقدم التطورات الحديثة في العلاج الجيني أملاً ...            Health"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the results\n",
    "result_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0540530-6bd5-457e-a289-e31ee4969248",
   "metadata": {},
   "source": [
    "Now, let's use the `MultilingualDomainClassifier` to process our previously filtered multilingual corpus (French and Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13cd91c6-6e1b-4fcf-903c-2ba1ce575c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n",
      "Starting multilingual domain classifier inference\n"
     ]
    }
   ],
   "source": [
    "# load the filtered data\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./datasets/multilingual/language_separation/language/EN/file.jsonl\"\n",
    "multilingual_dataset = DocumentDataset.read_json(multilingual_data_path, backend=\"cudf\")\n",
    "\n",
    "# Domain classification\n",
    "multilingual_result_domain = domain_classifier(dataset=multilingual_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2918da7-d4c9-4147-b35b-caf71cb132cf",
   "metadata": {},
   "source": [
    "Let's check the output. Expected to see an aditional field `domain_pred`:\n",
    "```\n",
    "text                                            \t\tdomain_pred\n",
    "Dragon Ball: Le 20e film de la sage sortira le...\t\tArts_and_Entertainment\n",
    "Cours D'histoire Des États Européens: Depuis L...\t\tBooks_and_Literature\n",
    "Se realizó una jornada de promoción del buentr...\t\tPeople_and_Society\n",
    "...\n",
    "```\n",
    "\n",
    "Execute the following cell to review the topic predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9d4bc41-0aa7-477c-9b7b-a61f8629c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU: tcp://127.0.0.1:42049, Part: 0: 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>domain_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>psa websites like labtestsonline.org, fengoffi...</td>\n",
       "      <td>1561389535000</td>\n",
       "      <td>http://www.statshow.com/tag/psa</td>\n",
       "      <td>Autos_and_Vehicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN</td>\n",
       "      <td>Una imagen – Estrella – En esa cierta edad\\nJu...</td>\n",
       "      <td>1550942284000</td>\n",
       "      <td>https://www.estrellaesteve.com/2009/07/una-ima...</td>\n",
       "      <td>Arts_and_Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN</td>\n",
       "      <td>Una imagen – Estrella – En esa cierta edad\\nJu...</td>\n",
       "      <td>1550942284000</td>\n",
       "      <td>https://www.estrellaesteve.com/2009/07/una-ima...</td>\n",
       "      <td>Arts_and_Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN</td>\n",
       "      <td>﻿ INCANDESCE : définition de INCANDESCE et syn...</td>\n",
       "      <td>1597298497000</td>\n",
       "      <td>http://dictionnaire.sensagent.leparisien.fr/IN...</td>\n",
       "      <td>People_and_Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN</td>\n",
       "      <td>Nos Vemos en el Camino - El Sueño de Morfeo（El...</td>\n",
       "      <td>1596719920000</td>\n",
       "      <td>https://music.163.com/album?id=255342</td>\n",
       "      <td>Arts_and_Entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                               text      timestamp  \\\n",
       "0       EN  psa websites like labtestsonline.org, fengoffi...  1561389535000   \n",
       "1       EN  Una imagen – Estrella – En esa cierta edad\\nJu...  1550942284000   \n",
       "2       EN  Una imagen – Estrella – En esa cierta edad\\nJu...  1550942284000   \n",
       "3       EN  ﻿ INCANDESCE : définition de INCANDESCE et syn...  1597298497000   \n",
       "4       EN  Nos Vemos en el Camino - El Sueño de Morfeo（El...  1596719920000   \n",
       "\n",
       "                                                 url             domain_pred  \n",
       "0                    http://www.statshow.com/tag/psa      Autos_and_Vehicles  \n",
       "1  https://www.estrellaesteve.com/2009/07/una-ima...  Arts_and_Entertainment  \n",
       "2  https://www.estrellaesteve.com/2009/07/una-ima...  Arts_and_Entertainment  \n",
       "3  http://dictionnaire.sensagent.leparisien.fr/IN...      People_and_Society  \n",
       "4              https://music.163.com/album?id=255342  Arts_and_Entertainment  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the domain classification\n",
    "multilingual_result_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22c32d-4775-4ce5-bfc9-d425a131e40a",
   "metadata": {},
   "source": [
    "Let's now save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10addeac-df75-4262-8ef0-1d7a787a2aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU: tcp://127.0.0.1:42049, Part: 0: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU: tcp://127.0.0.1:42049, Part: 0: 100%|██████████| 5/5 [00:02<00:00,  1.95it/s]\n",
      "/usr/local/lib/python3.12/dist-packages/cudf/io/json.py:382: UserWarning: Using CPU via Pandas to write JSON dataset\n",
      "  warnings.warn(\"Using CPU via Pandas to write JSON dataset\")\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "result_domain.to_json(domain_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54154522-e708-4011-8eba-df81108c284d",
   "metadata": {},
   "source": [
    "We can check the saved outputs by executing the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a68e47c-e938-4ad7-b7cc-01972ea22ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Il adore les chats.\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"domain_pred\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Pets_and_Animals\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 {domain_output_path}/0.part | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8810f-f7e8-4510-95ef-b29f97e904b0",
   "metadata": {},
   "source": [
    "## 2.3 Deduplication\n",
    "\n",
    "Document-level deduplication aims to reduce the occurrence of duplicate and near-duplicate documents in a dataset. This is crucial for datasets cleaning, reducing redundancy, and ensuring that models are trained on diverse and unique data.\n",
    "\n",
    "In this section, we will explore both the Exact and Fuzzy deduplication. Both functionalities are supported in NeMo Curator and accelerated using the [RAPIDS](https://rapids.ai/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188eeb3-4fbe-4eb2-8438-783e1a6ed1fe",
   "metadata": {},
   "source": [
    "\n",
    "Remember, we created our multilingual (Spanish and French) dataset by deduplicating each sample once.\n",
    "Before running deduplication, we need to ensure that each document in the dataset has a unique ID. We can use the `add_id` module within NeMo Curator to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59e23e-9db7-4f54-a764-d8f2801ca596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output folders\n",
    "from nemo_curator import AddId\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "!mkdir -p {data_dir}\n",
    "\n",
    "dataset_fr = DocumentDataset.read_json(\n",
    "    os.path.join(language_separated_output_path, \"FR/\"), add_filename=True\n",
    ")\n",
    "dataset_es = DocumentDataset.read_json(\n",
    "    os.path.join(language_separated_output_path, \"ES/\"), add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6077d-4373-43a3-b68a-cc4302a99c73",
   "metadata": {},
   "source": [
    "### 2.3.1 Add Unique ID\n",
    "\n",
    "Let's start by adding a unique ID for out dataset separated per language (Spanish and French)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d53f7-ca0b-4d87-a0d3-bdb060106fef",
   "metadata": {},
   "source": [
    "Let's run the `AddId` on the French corpus by running the next cell. The Format of output ID will be `<prefix>_<id>` where `prefix` is provided and `id` is a generated unique number. \n",
    "\n",
    "Let's apply the `AddId` function to the French corpus by running the next cell. The output ID format will be `<prefix>_<id>`, where `prefix` is specified by the user, and `id` is a uniquely generated number.\n",
    "\n",
    "\n",
    "Example of expected output:\n",
    "```\n",
    "text\t                                         \t\tid\n",
    "Dragon Ball: Le 20e film de la sage sortira le...\t\tFR_data-0000000000\n",
    "Cours D'histoire Des États Européens: Depuis L...\t\tFR_data-0000000001\n",
    "...\n",
    "```\n",
    "\n",
    "Execute the following cell to apply `AddId` to the French corpus, user prefix here is set to `FR_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488b70b-a4be-4fad-82b2-739e4e31b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define user's prefix\n",
    "FR_add_ID_id_prefix = \"FR_data\"\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=FR_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_fr = add_id(dataset_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f17f49-950d-417a-9dc0-86c097431a2c",
   "metadata": {},
   "source": [
    "Let's check the outputs. Notice the new field `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fbfc4-8b21-4ca9-9677-40df6e488645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outputs\n",
    "id_dataset_fr.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a455f9-30da-4aed-89a6-cbc4249aa455",
   "metadata": {},
   "source": [
    "We can save the outputs in their designated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9dff1-a8b7-4e38-bf4e-98804fd42c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dataset_fr.to_json(os.path.join(added_id_output_path, \"FR/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d9f6d-ae21-4db4-b43d-5bab02408d61",
   "metadata": {},
   "source": [
    "#### Exercice:  Add Unique ID for Spanish data.\n",
    "Make sure to replace the `# Your code here`. If you get stuck, refer to the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0051b-3ccb-4c42-9bc6-d98c7a59215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_add_ID_id_prefix = # Your code here\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=ES_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_es = # Your code here\n",
    "\n",
    "# save to relevant folder\n",
    "id_dataset_es.to_json(os.path.join(added_id_output_path, \"ES/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830c81b-6a15-4bcd-9036-3030cc044838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "ES_add_ID_id_prefix = \"ES_data\"\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=ES_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_es = add_id(dataset_es)\n",
    "\n",
    "# save to relevant folder\n",
    "id_dataset_es.to_json(os.path.join(added_id_output_path, \"ES/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0cdc8-5408-4cef-a462-3e92a8161f4e",
   "metadata": {},
   "source": [
    "### 2.3.2 Exact Deduplication\n",
    "\n",
    "Exact Deduplication consists in identifying and removing duplicate documents that are exactly identical within a dataset. This process helps eliminate redundant data, prevents models from overfitting on repeated examples, and ensures that training and test sets do not contain the same samples, which could otherwise lead to misleading evaluation metrics.\n",
    "\n",
    "In [NeMo Curator](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html), exact deduplication works by hashing each document and keeping only one document per hash, and it can be run on both GPU ([CuDF](https://docs.rapids.ai/api/cudf)) and CPU ([Pandas](https://pandas.pydata.org/)) based backends.\n",
    "\n",
    "\n",
    "Let's create the folders for the exact deduplication. We will save the output results in `/data`, temporary files in `/cache`, and logs in `/log`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066f0a2-cd4b-4440-b49f-9444e246ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_es = \"curated/07_Deduplicate/exact/ES\"\n",
    "\n",
    "exact_dedup_log_dir_es = os.path.join(data_dir_es, \"log\")\n",
    "exact_dedup_cache_dir_es = os.path.join(data_dir_es, \"cache\")\n",
    "exact_dedup_output_dir_es = os.path.join(data_dir_es, \"data\")\n",
    "\n",
    "# Create all required directories\n",
    "os.makedirs(exact_dedup_log_dir_es, exist_ok=True)\n",
    "os.makedirs(exact_dedup_cache_dir_es, exist_ok=True)\n",
    "os.makedirs(exact_dedup_output_dir_es, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6fff9-aa31-4bee-a770-8848f6868d82",
   "metadata": {},
   "source": [
    "Before running exact deduplication in NeMo Curator, the dataset needs to present a unique ID for each document (sample). We already added these unique IDs in the previous step in the field `\"id\"`.\n",
    "\n",
    "We will be running the exact deduplication on the GPU using cudf backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87637c3b-5700-44b4-994f-6854f28db506",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_field = \"id\"\n",
    "input_dataset_es = DocumentDataset.read_json(\n",
    "    os.path.join(added_id_output_path, \"ES/\"), backend=\"cudf\", add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035c15f-e01f-464b-abff-a11f418fb6a0",
   "metadata": {},
   "source": [
    "Execute the next cell to run the exact deduplication on the Spanish dataset. This should take about 10 seconds to process.\n",
    "\n",
    "We can use `perform_removal=True` to apply the duplicate removal directly on the dataset. But, for the sake of this exercise, we will first show the deduplication identifification before actually applying the removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fc88f-cdac-414d-887e-19fc815bcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nemo_curator.modules import ExactDuplicates\n",
    "\n",
    "# run exact deducplicate\n",
    "exact_dup_es = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir_es,\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_cache_dir_es,\n",
    ")\n",
    "duplicates_es = exact_dup_es(dataset=input_dataset_es)\n",
    "exact_docs_to_remove_es = duplicates_es.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b237f77-8155-48e4-99bd-1285c7584711",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ed545-0511-45bc-8a9a-0576ae413222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data:{len(input_dataset_es)}\")\n",
    "print(f\"Number of documents to be removed:{len(exact_docs_to_remove_es)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5291954-f400-416a-8024-21c55309f7ee",
   "metadata": {},
   "source": [
    "Check some duplicate documents: \n",
    "\n",
    "Example of output: \n",
    "```\n",
    "     id                  _hashes\n",
    "18   ES_data-0000000146 2f610eed57653fbe68328fbaf3274c2a\n",
    "20   ES_data-0000000148  e473009ec2e1a246de93fea08488ca4c\n",
    "21   ES_data-0000000149  066347c8a96bc73056a9f172e4d9710\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f26eec-c730-438a-8e0a-986482095294",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_docs_to_remove_es.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6e26f-4dd5-47b7-92a7-d7ff66f5cbcc",
   "metadata": {},
   "source": [
    "Now, apply the deduplication removal and save the results to the output data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955713b8-c1b7-41e5-9a77-e6c357624164",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_es = input_dataset_es.df[\n",
    "    ~input_dataset_es.df[id_field].isin(exact_docs_to_remove_es[id_field].compute())\n",
    "]\n",
    "DocumentDataset(result_es).to_json(exact_dedup_output_dir_es, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9a68c-f952-4c98-9b02-d521bb60ea93",
   "metadata": {},
   "source": [
    "Check saved output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb8df5-aee1-4f2b-b1d3-90c40e1a9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 {exact_dedup_output_dir_es}/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c786ff-f2ea-459b-afe1-9b57044722b6",
   "metadata": {},
   "source": [
    "#### Exercice: Run Exact Desuplication for the French data.\n",
    "\n",
    "Run the same exact deduplication for the French data. \n",
    "\n",
    "Let's first create the relevant folders and set the dataset and id field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefbd1d-e72f-492f-953b-3d384184d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_fr = \"curated/07_Deduplicate/exact/FR\"\n",
    "\n",
    "exact_dedup_log_dir_fr = os.path.join(data_dir_fr, \"log\")\n",
    "exact_dedup_cache_dir_fr = os.path.join(data_dir_fr, \"cache\")\n",
    "exact_dedup_output_dir_fr = os.path.join(data_dir_fr, \"data\")\n",
    "!mkdir -p {exact_dedup_log_dir_fr}\n",
    "!mkdir -p {exact_dedup_cache_dir_fr}\n",
    "!mkdir -p {exact_dedup_output_dir_fr}\n",
    "\n",
    "id_field = \"id\"\n",
    "input_dataset_fr = DocumentDataset.read_json(\n",
    "    os.path.join(added_id_output_path, \"FR/\"), backend=\"cudf\", add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80423c61-dcea-4667-9a28-65bbf8a8d113",
   "metadata": {},
   "source": [
    "Run the deduplication. Make sure to replace the `# Your code here`. If you get stuck, refer to the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011c0af-0d20-42c5-a7c8-a6a2c503b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run exact deduplicate\n",
    "exact_dup_fr = # Your code here\n",
    "duplicates_fr = # Your code here\n",
    "exact_docs_to_remove_fr = # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d0b1f-efae-4188-9004-36d173c63128",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "# run exact deducplicate\n",
    "exact_dup_fr = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir_fr,\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_cache_dir_fr,\n",
    ")\n",
    "\n",
    "duplicates_fr = exact_dup_fr(dataset=input_dataset_fr)\n",
    "exact_docs_to_remove_fr = duplicates_fr.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34d96c-23fb-47b5-8988-4f25b50bafc6",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cf7c8-4808-4f5d-9d1b-5e1a9f3191d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data:{len(input_dataset_fr)}\")\n",
    "print(f\"Number of documents to be removed:{len(exact_docs_to_remove_fr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c9bb6-d4c8-4251-8559-2db39cf5d2ac",
   "metadata": {},
   "source": [
    "Now, apply the deduplication removal and save the results to the output data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db1055-26e3-47a4-a24d-f401af247652",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fr = input_dataset_fr.df[\n",
    "    ~input_dataset_fr.df[id_field].isin(exact_docs_to_remove_fr[id_field].compute())\n",
    "]\n",
    "DocumentDataset(result_fr).to_json(exact_dedup_output_dir_fr, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78744bc-547d-4d81-bd7d-573a8e31bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d198c5-0c7a-4b44-889d-7c2de5d80ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa300f62-3410-4ff8-9bc0-286b46ea2b70",
   "metadata": {},
   "source": [
    "### 2.3.3 Fuzzy Deduplication\n",
    "\n",
    "Removing near-duplicates is referred to as fuzzy deduplication at the document level, which is based on Jaccard similarity scores.\n",
    "\n",
    "This approach can be broken down into the following stages:\n",
    "- **Stage 1 - Minhash + LSH:** The first step involves generating MinHash signatures for the documents. NeMo Curator currently supports character-based n-grams for MinHashing. Then, the Locality Sensitive Hashing (LSH) is performed to identify candidate duplicates.\n",
    "- **Stage 2 - LSH Buckets to Graph edgelist:** LSH buckets are directly converted to edges for the connected components computation.\n",
    "- **Stage 3 - Connect Components:** Since LSH is an approximate method, documents that are near duplicates may end up in different buckets, with some overlapping documents between them. A GPU-accelerated connected components algorithm is used to identify all connected components in the graph formed by the edges between documents within the same bucket. The output of this step is a list of document IDs and the groups they belong to.\n",
    "\n",
    "All documents within the same group are considered near duplicates, and results can then be used to remove them from the corpus.\n",
    "For more information, refer to the Deduplication documentation of [NeMo Curator](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html).\n",
    "\n",
    "\n",
    "There are no near-duplicates in out example datasets. However, to demonstrate the process, let's run fuzzy deduplication on the French dataset and go through the steps involved.\n",
    "\n",
    "Let's create fisrt the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3cc93-65fc-4467-b4a4-f5cd26682c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fuzzy_dedup_log_dir_fr = \"curated/07_Deduplicate/fuzzy_wrapper/FR\"\n",
    "os.makedirs(fuzzy_dedup_log_dir_fr, exist_ok=True)\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "os.makedirs(added_id_output_path, exist_ok=True)  # Creates \"curated/06_add_id/add_id/cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b998c6-62c1-4b24-9f60-9462410ee43e",
   "metadata": {},
   "source": [
    "Let's start the Dask client. Make sure that you have stopped the previous one before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c1970-36cf-4e11-a80a-8a0ebe331696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from nemo_curator.utils.import_utils import gpu_only_import, gpu_only_import_from\n",
    "\n",
    "cudf = gpu_only_import(\"cudf\")\n",
    "dask_cudf = gpu_only_import(\"dask_cudf\")\n",
    "LocalCUDACluster = gpu_only_import_from(\"dask_cuda\", \"LocalCUDACluster\")\n",
    "\n",
    "cluster = LocalCUDACluster(n_workers=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a25539-2c7a-4338-a0ac-60912b739d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDF_SPILL\"] = \"on\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4796c6-1002-4692-bcc3-6e58f5b30aa9",
   "metadata": {},
   "source": [
    "We will use the `FuzzyDuplicates` method from NeMo Curator to run the fuzzy deduplication process on the French dataset. This will allow us to identify and handle any near-duplicates based on similarity scores.\n",
    "\n",
    "You should see the three stages logged during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cd642-b865-405e-a51f-ffa0c6bb505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dedup_log_dir_fr = \"curated/07_Deduplicate/fuzzy_wrapper/FR\"\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "input_fr = os.path.join(added_id_output_path, \"FR/file.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc1758-894d-44d6-a3fe-a6ae057820c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import FuzzyDuplicates, FuzzyDuplicatesConfig\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "config = FuzzyDuplicatesConfig(\n",
    "    cache_dir=fuzzy_dedup_log_dir_fr,  # must be cleared between runs\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    num_buckets=20,\n",
    "    hashes_per_bucket=13,\n",
    "    use_64_bit_hash=False,\n",
    "    buckets_per_shuffle=2,\n",
    "    false_positive_check=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the deduplication object\n",
    "FuzzyDups = FuzzyDuplicates(config=config, logger=\"./\")\n",
    "\n",
    "# load the dataset\n",
    "dataset_fr = DocumentDataset.read_json(\n",
    "    input_files=input_fr,\n",
    "    backend=\"cudf\",  # FuzzyDuplicates only supports datasets with the cuDF backend.\n",
    ")\n",
    "\n",
    "# run Fuzzy Duplicate\n",
    "duplicate_docs = FuzzyDups(dataset_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56671fc-d54d-4bb7-80aa-c03ed578c2da",
   "metadata": {},
   "source": [
    "The result from the connected components stage is a list of document IDs and the group they belong to. All documents in the same group are considered near duplicates. \n",
    "\n",
    "```\n",
    "id\t                group\n",
    "FR_data-0000000062\t46\n",
    "FR_data-0000000013\t47\n",
    "FR_data-0000000104\t160\n",
    "FR_data-0000000185\t161\n",
    "FR_data-0000000155\t65\n",
    "...\n",
    "```\n",
    "Let's check the outputs. Notice the `group` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc464a1-54a9-48ab-b2af-b523f34b5a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_docs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baad0c2-827f-4664-bc89-70a5f3432340",
   "metadata": {},
   "source": [
    "These groups can be then used to remove the near duplicates from the corpus.\n",
    "\n",
    "Let's run that by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2160e29-08b2-4665-9827-b8f5e42275dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_remove = duplicate_docs.df.map_partitions(\n",
    "    lambda x: x[x.group.duplicated(keep=\"first\")]\n",
    ")\n",
    "result = dataset_fr.df[~dataset_fr.df[\"id\"].isin(docs_to_remove[\"id\"].compute())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbfc1b-a174-4fcd-9475-768c260f7739",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835bb183-790f-48d5-aeb7-e6e3fd66236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data : {len(dataset_fr)}\")\n",
    "print(f\"Number of documents to be removed : {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff85b50-bcb5-4419-9cd9-9d98834bb997",
   "metadata": {},
   "source": [
    "#### [Optional] Explore further Deduplication on downstream tasks\n",
    "\n",
    "Large Language Models are typically evaluated based on their performance on downstream tasks using unseen test data. However, when working with extensive datasets, there is a risk of test data leaking into the model's training set. \n",
    "\n",
    "To mitigate this, NeMo Curator provides a Decontamination strategy, in order to ensure that any document sections appearing in downstream tasks are removed from the training set. \n",
    "\n",
    "You can explore this in more detail in the [task decontamination](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/taskdecontamination.html) of NeMo Curator documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076bdf0-1e24-4301-8397-2bd44c26c187",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "\n",
    "In this notebook, you have used NeMo Curator to apply several data cleaning steps, including language detection and filtering, topic classification and document deduplication. These steps help ensure that the dataset is clean, diverse, and free from redundant data, improving the quality of the data used for training and evaluation.\n",
    "\n",
    "Before moving on to the next notebook, make sure to stop the Dask cluster. Please run the next cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
