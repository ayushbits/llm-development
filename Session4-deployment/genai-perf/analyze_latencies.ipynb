{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Benchmarks Visualiazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, fixed, IntSlider, IntText\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, prefix=\"\", new_d=None, separator=\"-\"):\n",
    "    if new_d is None: new_d = {}\n",
    "    for k, v in d.items():\n",
    "        new_k = prefix + separator + k if prefix else k\n",
    "        if isinstance(v, dict):\n",
    "            new_d = flatten_dict(v, new_k, new_d, separator)\n",
    "        else:\n",
    "            new_d[new_k] = v\n",
    "    return new_d\n",
    "\n",
    "def format_value(v):\n",
    "    if isinstance(v, int):\n",
    "        return f\"{v:,}\"\n",
    "    elif isinstance(v, float):\n",
    "        if np.isnan(v): return \"N/A\"\n",
    "        if np.abs(v) >= 1 and np.abs(v) <= 1e6:\n",
    "            return f\"{v:,.1f}\"\n",
    "        return f\"{v:,.2g}\"\n",
    "    else: return f\"{v}\"\n",
    "\n",
    "\n",
    "\n",
    "def load_latencies_nim_pbr():\n",
    "    files = glob.glob(\"*genai-perf*/**/*genai_perf.json\", recursive=True)\n",
    "    data = []\n",
    "\n",
    "    # Example path I have\n",
    "    # genai-perf-artifacts/model-meta-llama-3.1-8b-instruct/profile-tensorrt_llm_buildable-h200-bf16-tp4-dp1-throughput/2025-07-29_16.33.09/concurrency-50/meta-llama-3.1-8b-instruct-openai-chat-concurrency50/200_200_genai_perf.json    \n",
    "    # # TODO: Modify regex to match your path structure\n",
    "\n",
    "    fn_regex = re.compile(r'[./\\\\]*?genai-perf-.*[/\\\\]'\n",
    "                        +r'model-(?P<model>.*?)[/\\\\]'\n",
    "                        +r'profile-(?P<profile>.*?)[/\\\\]'\n",
    "                        +r'(?P<datetime>.*?)[/\\\\]'\n",
    "                        +r'concurrency-(?P<concurrency>\\d+)[/\\\\]'\n",
    "                        +r'(?P<measurement_label>.*?[/\\\\])?' # introduced by the 25.06 GenAI-Perf\n",
    "                        +r'(?P<input_len>\\d+)_(?P<output_len>\\d+)_genai_perf.json'\n",
    "                        )\n",
    "\n",
    "    profile_regex = re.compile(r'(?P<engine>vllm|tensorrt_llm|tensorrt_llm_buildable)'\n",
    "                        +r'(-(?P<device>.*?))?'\n",
    "                        +r'-(?P<precision>[^-]*)'\n",
    "                        +r'(-tp(?P<TP>\\d+))?'\n",
    "                        +r'(-pp(?P<PP>\\d+))?'\n",
    "                        +r'(-dp(?P<DP>\\d+))?'\n",
    "                        +r'(-(?P<tuning>throughput|latency))?'\n",
    "                        +r'(-(?P<lora>lora))?'\n",
    "                        )\n",
    "\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as jf:\n",
    "            l = json.load(jf)\n",
    "\n",
    "            # match = re.search(fn_regex, l[\"input_config\"][\"profile_export_file\"])\n",
    "            match = re.search(fn_regex, f)\n",
    "            task_inputs = match.groupdict()\n",
    "\n",
    "            # print(task_inputs[\"profile\"])\n",
    "            profile = {}\n",
    "            try:\n",
    "                profile_match = re.search(profile_regex, task_inputs[\"profile\"])\n",
    "                profile = profile_match.groupdict()\n",
    "                # print(f\"{task_inputs['profile']=}\")\n",
    "                # print(f\"{profile=}\")\n",
    "            except:\n",
    "                print(f\"WARNING: Failed to parse profile from {task_inputs['profile']}\")\n",
    "                continue\n",
    "\n",
    "            l[\"task_inputs\"] = {**task_inputs, **profile}\n",
    "            l_flattened = flatten_dict(l)\n",
    "            data += [l_flattened]\n",
    "\n",
    "    df_raw = pd.DataFrame(data)\n",
    "    return df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df_raw = load_latencies_nim_pbr()\n",
    "print(len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "# Rename columns of raw csv file\n",
    "df[\"model\"] = df_raw['task_inputs-model']\n",
    "df[\"device\"] = df_raw.get('task_inputs-device', \"some-GPU\")\n",
    "df[\"engine\"] = df_raw.get('task_inputs-engine', \"some-engine\")\n",
    "df[\"profile\"] = df_raw.get('task_inputs-profile', \"some-profile\")\n",
    "df[\"datetime\"] = df_raw['task_inputs-datetime']\n",
    "df[\"TP\"] = pd.to_numeric(df_raw.get('task_inputs-TP', -1), errors='coerce').fillna(-1).astype(int)\n",
    "df[\"n_nims\"] = pd.to_numeric(df_raw.get('task_inputs-DP', 1), errors='coerce').fillna(1).astype(int)\n",
    "df[\"PP\"] = pd.to_numeric(df_raw.get('task_inputs-PP', 1), errors='coerce').fillna(1).astype(int)\n",
    "df[\"out_tokens_per_s\"] = df_raw['output_token_throughput-avg']\n",
    "df[\"latency_first_token\"] = df_raw['time_to_first_token-avg'] / 1 # ms → ms\n",
    "df[\"latency_per_token_decoding\"] = df_raw['inter_token_latency-avg'] / 1 # ms → ms\n",
    "df[\"input_len\"] = df_raw['input_config-input-synthetic_tokens-mean']\n",
    "df[\"output_len\"] = df_raw['input_config-input-output_tokens-mean']\n",
    "\n",
    "df[\"batch_size\"] = np.nan\n",
    "df[\"concurrency\"] = df_raw['input_config-perf_analyzer-stimulus-concurrency']\n",
    "df[\"precision\"] = df_raw.get(\"task_inputs-precision\", \"some-precision\")\n",
    "df[\"prompts_per_s\"] = df_raw['request_throughput-avg']\n",
    "df[\"latency\"] = df_raw['request_latency-avg'] / 1\n",
    "df[\"out_tokens_per_s_per_user\"] = df_raw['output_token_throughput_per_user-avg']\n",
    "\n",
    "df[\"n_gpus\"] = df[\"TP\"] * df[\"n_nims\"]\n",
    "\n",
    "df[\"input_output_len\"] = df[\"input_len\"].astype(str) + \" in → \" + df[\"output_len\"].astype(str) + \" out\"\n",
    "df[\"prompts_per_s_per_gpu\"] = df[\"prompts_per_s\"] / df[\"n_gpus\"]\n",
    "df[\"prompts_per_s_per_8_gpus\"] = df[\"prompts_per_s_per_gpu\"] * 8\n",
    "df[\"out_tokens_per_s_per_gpu\"] = df[\"out_tokens_per_s\"] / df[\"n_gpus\"]\n",
    "df[\"out_tokens_per_s_per_8_gpus\"] = df[\"out_tokens_per_s_per_gpu\"]  * 8 \n",
    "df[\"batch_size_per_8\"] = df[\"batch_size\"] * 8 / df[\"n_gpus\"]\n",
    "df[\"concurrency_per_8\"] = df[\"concurrency\"] * 8 / df[\"n_gpus\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_fixed_lengths(df, model, device, input_output_len, x_metric=\"latency\", y_metric=\"prompts_per_s_per_gpu\", include_costs=False):\n",
    "    df_measured = df.iloc[::-1]\n",
    "    filters = {\n",
    "        \"model\": model,\n",
    "        \"input_output_len\": input_output_len,\n",
    "        }\n",
    "    df_measured = df_measured[df_measured[list(filters)].eq(pd.Series(filters)).all(axis=1)]\n",
    "    index_columns = [\"profile\", \"n_nims\", \"datetime\"]\n",
    "    hover_fields = [\"TP\", \"latency\", \"concurrency\", \"concurrency_per_8\", \"input_len\", \"output_len\"]\n",
    "    if include_costs:\n",
    "         hover_fields = [\"planned_prompts_per_second\", \"on_prem_cost_per_1M_input_tokens\", \"on_prem_cost_per_1M_output_tokens\", \"min_servers_required\", \"min_gpus_required\"] + hover_fields\n",
    "    index_set = list(df_measured[index_columns].groupby(index_columns, dropna=False).first().index)\n",
    "    index_set.reverse()\n",
    "    def get_hover(df_iterator):\n",
    "            return [\n",
    "                        (\n",
    "                            \"</br>\"\n",
    "                            + \" </br>\".join(f\"{f}: {format_value(row[f])}\" for f in hover_fields)\n",
    "                            +\"\",\n",
    "                            dict(row),\n",
    "                        )\n",
    "                        for i, row in df_iterator\n",
    "                    ]\n",
    "    def get_name(filters):\n",
    "            name = []\n",
    "            for k, v in filters.items():\n",
    "                if k in [\"execution_mode\", \"precision\", \"datetime\", \"device\", \"profile\"]:\n",
    "                    name += [f\"{v}\"]\n",
    "                elif v != v: # v is nan\n",
    "                    continue\n",
    "                else:\n",
    "                    name += [f\"{k} {v}\"]\n",
    "            return \", \".join(name)\n",
    "    def compare_with_nan(series, value):\n",
    "            if pd.isna(value):\n",
    "                return series.isna()\n",
    "            else:\n",
    "                return series == value\n",
    "    fig = make_subplots()\n",
    "    for i, index_value in enumerate(index_set):\n",
    "        filters = { k: v for k, v in zip(index_columns, index_value) }\n",
    "        boolean_series_list = [compare_with_nan(df_measured[col], val) for col, val in filters.items()]\n",
    "        df_filtered = df_measured[np.logical_and.reduce(boolean_series_list)].sort_values(by=\"concurrency\")\n",
    "        hover = get_hover(df_filtered.iterrows())\n",
    "        trace = go.Scatter(\n",
    "            x = df_filtered[x_metric],\n",
    "            y = df_filtered[y_metric],\n",
    "            name = get_name(filters),\n",
    "            customdata = hover,\n",
    "            mode = 'lines+markers',\n",
    "            hovertemplate = \"%{customdata[0]}\",\n",
    "            marker={\n",
    "                \"size\": 12,\n",
    "                \"color\": px.colors.qualitative.G10[i % 10],\n",
    "                \"opacity\": 0.7\n",
    "            },\n",
    "        )\n",
    "        fig.add_trace(trace)\n",
    "    fig.update_xaxes(title_text=x_metric, type=\"log\")\n",
    "    fig.update_yaxes(title_text=y_metric)\n",
    "    title=f\"{model}, {device}, tokens: {input_output_len}\"\n",
    "    if include_costs:\n",
    "        planned_prompts_per_second = df.iloc[0][\"planned_prompts_per_second\"]\n",
    "        title += f\", {planned_prompts_per_second=:.2f}\"\n",
    "    fig.update_layout(title=title)\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2025-09-09_18.27.06']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df[\"datetime\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2145824a26b44bda790eebbb9032bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('meta-llama-3.1-8b-instruct',), value='meta-llama…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.scatter_fixed_lengths(df, model, device, input_output_len, x_metric='latency', y_metric='prompts_per_s_per_gpu', include_costs=False)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(scatter_fixed_lengths, \n",
    "         df = fixed(df),\n",
    "         model = df['model'].unique(),\n",
    "         device = df['device'].unique(),\n",
    "         input_output_len = df['input_output_len'].unique(),\n",
    "         x_metric = [\"latency_per_token_decoding\", \"latency_first_token\", \"latency\"],\n",
    "         y_metric = [\"out_tokens_per_s\", \"prompts_per_s\", \"prompts_per_s_per_gpu\", \"out_tokens_per_s_per_user\", \"out_tokens_per_s_per_gpu\"],\n",
    "         include_costs=fixed(False), \n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's estimate the TCO. Please fill in the costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"on_prem_server_cost\"] = 300_000 # the CapEx per server in USD\n",
    "df[\"server_depreciation_years\"] = 4 \n",
    "df[\"hosting_cost_per_server_per_year\"] = 0 # per year in USD\n",
    "df[\"nvaie_license_per_gpu_per_year\"] = 4500 # per year in USD\n",
    "df[\"on_prem_gpus_per_server\"] = 8\n",
    "df[\"cloud_api_cost_per_1M_input_tokens\"] = 1 # USD\n",
    "df[\"cloud_api_cost_per_1M_output_tokens\"] = 3 # USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tco_metrics(df, planned_prompts_per_second):\n",
    "    df[\"output_to_input_token_cost_ratio\"] = df[\"cloud_api_cost_per_1M_output_tokens\"] / df[\"cloud_api_cost_per_1M_input_tokens\"]\n",
    "    df[\"planned_prompts_per_second\"] = planned_prompts_per_second\n",
    "    df[\"min_instances_required\"] = planned_prompts_per_second/ df[\"prompts_per_s\"]\n",
    "    df[\"min_servers_required\"] = df[\"min_instances_required\"] / np.floor_divide(df[\"on_prem_gpus_per_server\"], df[\"n_gpus\"])\n",
    "    df[\"min_gpus_required\"] = df[\"min_instances_required\"] * df[\"n_gpus\"]\n",
    "    df[\"on_prem_costs_per_year_per_server\"] = \\\n",
    "        df[\"nvaie_license_per_gpu_per_year\"] * df[\"on_prem_gpus_per_server\"]\\\n",
    "        + df[\"hosting_cost_per_server_per_year\"] \\\n",
    "        + df[\"on_prem_server_cost\"] / df[\"server_depreciation_years\"]\n",
    "    df[\"on_prem_costs_per_year_for_min_servers\"] = \\\n",
    "        df[\"nvaie_license_per_gpu_per_year\"] * df[\"planned_prompts_per_second\"] / df[\"prompts_per_s_per_gpu\"]\\\n",
    "        + df[\"hosting_cost_per_server_per_year\"] * df[\"min_servers_required\"] \\\n",
    "        + df[\"on_prem_server_cost\"] * df[\"min_servers_required\"] / df[\"server_depreciation_years\"]\n",
    "    df[\"on_prem_costs_per_day_per_server\"] = df[\"on_prem_costs_per_year_per_server\"] / 365.\n",
    "    df[\"prompts_per_s_per_server\"] = df[\"prompts_per_s\"] * np.floor_divide(df[\"on_prem_gpus_per_server\"], df[\"n_gpus\"])\n",
    "    df[\"prompts_per_day_per_server\"] = df[\"prompts_per_s_per_server\"] * 60 * 60 * 24 \n",
    "    df[\"on_prem_cost_per_prompt\"] = df[\"on_prem_costs_per_day_per_server\"] / df[\"prompts_per_day_per_server\"]\n",
    "    df[\"on_prem_cost_per_1k_prompts\"] = df[\"on_prem_cost_per_prompt\"] * 1000\n",
    "    # Now we have \n",
    "    # ( cost_per_input_token * input_length  + cost_per_output_token * output_length ) * 1000 = df[\"on_prem_cost_per_1k_prompts\"]\n",
    "    # and cost_per_output_token = df[\"output_to_input_token_cost_ratio\"] * cost_per_input_token\n",
    "    # thus\n",
    "    # ( cost_per_input_token * input_length  + df[\"output_to_input_token_cost_ratio\"] * cost_per_input_token * output_length ) * 1000 = df[\"on_prem_cost_per_1k_prompts\"]\n",
    "    # thus\n",
    "    # cost_per_input_token * ( input_length  + df[\"output_to_input_token_cost_ratio\"] * output_length ) * 1000 = df[\"on_prem_cost_per_1k_prompts\"]\n",
    "    # thus\n",
    "    df[\"on_prem_cost_per_input_token\"] = df[\"on_prem_cost_per_1k_prompts\"] / ( df[\"input_len\"]  + df[\"output_to_input_token_cost_ratio\"] * df[\"output_len\"] ) / 1000 \n",
    "    df[\"on_prem_cost_per_1M_input_tokens\"] = df[\"on_prem_cost_per_input_token\"] * 1_000_000\n",
    "    df[\"on_prem_cost_per_1M_output_tokens\"] = df[\"on_prem_cost_per_1M_input_tokens\"] * df[\"output_to_input_token_cost_ratio\"]\n",
    "\n",
    "    df[\"cloud_api_cost_per_1M_input_prompts\"] = df[\"cloud_api_cost_per_1M_input_tokens\"] * df['input_len']\n",
    "    df[\"cloud_api_cost_per_1M_output_prompts\"] = df[\"cloud_api_cost_per_1M_output_tokens\"] * df['output_len']\n",
    "    df[\"cloud_api_cost_per_1M_prompts\"] = df[\"cloud_api_cost_per_1M_input_prompts\"] + df[\"cloud_api_cost_per_1M_output_prompts\"]\n",
    "    df[\"cloud_api_cost_per_1k_prompts\"] = df[\"cloud_api_cost_per_1M_prompts\"] / 1000\n",
    "    df[\"on_prem_to_cloud_api_cost_ratio\"] = df[\"on_prem_cost_per_1k_prompts\"] / df[\"cloud_api_cost_per_1k_prompts\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_different_output_length(df, new_output_length):\n",
    "    prev_output_length = df['output_len']\n",
    "    ratio = new_output_length / prev_output_length\n",
    "    latency_metrics = [ 'latency_first_token', 'latency']\n",
    "    for metric in latency_metrics:\n",
    "        df[metric] *= ratio\n",
    "    throughput_metrics = ['out_tokens_per_s',\n",
    "        'prompts_per_s', 'out_tokens_per_s_per_user',\n",
    "        'prompts_per_s_per_gpu',\n",
    "        'prompts_per_s_per_8_gpus', 'out_tokens_per_s_per_gpu',\n",
    "        'out_tokens_per_s_per_8_gpus'\n",
    "    ]\n",
    "    for metric in throughput_metrics:\n",
    "        df[metric] /= ratio\n",
    "    df['output_len'] = new_output_length\n",
    "    df[\"input_output_len\"] = df[\"input_len\"].astype(str) + \" in → \" + df[\"output_len\"].astype(str) + \" out\"\n",
    "    return df\n",
    "\n",
    "def add_simulated_output_lengths(df, new_output_lengths):\n",
    "    df_orig = df.copy()\n",
    "    df = df.copy()\n",
    "    for l in new_output_lengths:\n",
    "        df = pd.concat([df, simulate_different_output_length(df_orig.copy(), l)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example\n",
    "\n",
    "Expected usage is 1 million requests per month to LLM Application per use case. <br>\n",
    "We need to convert that number to number of Requests per second at peak load on the system.<br>\n",
    "We will calculate the average requests per second, and then multiply by the ratio of **average to peak** (current default is 3x). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "requests_per_minute_avg=23.15\n",
      "requests_per_second_avg=0.39\n",
      "requests_per_second_peak=1.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requests_per_month = 1_000_000\n",
    "ratio_peak_to_avg = 3 # change this to your expected ration\n",
    "\n",
    "seconds_per_month = 30 * 24 * 60 * 60\n",
    "requests_per_second_avg = requests_per_month / seconds_per_month\n",
    "requests_per_minute_avg = requests_per_second_avg * 60\n",
    "requests_per_second_peak = ratio_peak_to_avg * requests_per_second_avg\n",
    "print(f\"\"\"\n",
    "{requests_per_minute_avg=:.2f}\n",
    "{requests_per_second_avg=:.2f}\n",
    "{requests_per_second_peak=:.2f}\n",
    "\"\"\")\n",
    "\n",
    "df_costs = calculate_tco_metrics(df.copy(), requests_per_second_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37824598709c44f1af3f5e61885c071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('meta-llama-3.1-8b-instruct',), value='meta-llama…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.scatter_fixed_lengths(df, model, device, input_output_len, x_metric='latency', y_metric='prompts_per_s_per_gpu', include_costs=False)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(scatter_fixed_lengths, \n",
    "         df = fixed(df_costs),\n",
    "         model = df_costs['model'].unique(),\n",
    "         device = df_costs['device'].unique(),\n",
    "         input_output_len = df_costs['input_output_len'].unique(),\n",
    "         x_metric = [\"latency_per_token_decoding\", \"latency_first_token\", \"latency\"],\n",
    "         y_metric = [\"min_servers_required\", \"on_prem_cost_per_1M_output_tokens\", \"prompts_per_s_per_server\", \"prompts_per_s_per_gpu\", \"min_gpus_required\"],\n",
    "         include_costs=fixed(True),\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
