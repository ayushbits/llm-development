{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ NIM Performance Analysis\n",
        "\n",
        "This notebook shows:\n",
        "- **Concurrency vs RPS, TTFT, and ITL**\n",
        "- **GPU requirements for target performance**\n",
        "- **Interactive sliders that actually work**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from ipywidgets import interact, IntSlider, Dropdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ðŸ“š Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed data loading function for complex GenAI-Perf structure\n",
        "def load_data():\n",
        "    files = glob.glob(\"*genai-perf*/**/*genai_perf.json\", recursive=True)\n",
        "    \n",
        "    if not files:\n",
        "        print(\"âŒ No benchmark files found!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"ðŸ“ Found {len(files)} benchmark files\")\n",
        "    \n",
        "    # More comprehensive regex for the actual path structure\n",
        "    # genai-perf-artifacts/model-meta-llama-3.1-8b-instruct/profile-tensorrt_llm-trtllm_buildable-bf16-tp4-pp1/2025-09-09_18.27.06/concurrency-1/meta_llama-3.1-8b-instruct-openai-chat-concurrency1/200_200_genai_perf.json\n",
        "    path_pattern = r'model-([^/]+)/profile-([^/]+)/([^/]+)/concurrency-(\\d+)/[^/]+/(\\d+)_(\\d+)_genai_perf\\.json'\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for file in files:\n",
        "        try:\n",
        "            print(f\"Processing: {file}\")\n",
        "            \n",
        "            # Extract metadata from path\n",
        "            path_match = re.search(path_pattern, file)\n",
        "            if not path_match:\n",
        "                print(f\"   âš ï¸ Path doesn't match pattern: {file}\")\n",
        "                continue\n",
        "                \n",
        "            model = path_match.group(1)\n",
        "            profile = path_match.group(2)\n",
        "            datetime = path_match.group(3)\n",
        "            concurrency = int(path_match.group(4))\n",
        "            input_tokens = int(path_match.group(5))\n",
        "            output_tokens = int(path_match.group(6))\n",
        "            \n",
        "            # Load JSON data\n",
        "            with open(file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            \n",
        "            # Extract key metrics with proper error handling\n",
        "            result = {\n",
        "                'model': model,\n",
        "                'profile': profile,\n",
        "                'datetime': datetime,\n",
        "                'concurrency': concurrency,\n",
        "                'input_tokens': input_tokens,\n",
        "                'output_tokens': output_tokens,\n",
        "                'token_config': f\"{input_tokens}â†’{output_tokens}\",\n",
        "                'rps': data.get('request_throughput', {}).get('avg', 0),\n",
        "                'ttft_ms': data.get('time_to_first_token', {}).get('avg', 0),\n",
        "                'itl_ms': data.get('inter_token_latency', {}).get('avg', 0),\n",
        "                'tokens_per_sec': data.get('output_token_throughput', {}).get('avg', 0),\n",
        "                'total_latency_ms': data.get('request_latency', {}).get('avg', 0)\n",
        "            }\n",
        "            \n",
        "            print(f\"   âœ… Loaded: Concurrency {concurrency}, {input_tokens}â†’{output_tokens}, RPS {result['rps']:.1f}\")\n",
        "            results.append(result)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Error processing {file}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    if df.empty:\n",
        "        print(\"âŒ No valid data loaded\")\n",
        "        return df\n",
        "    \n",
        "    # Add derived metrics\n",
        "    df['gpus_used'] = 4  # From your NIM setup\n",
        "    df['rps_per_gpu'] = df['rps'] / df['gpus_used']\n",
        "    \n",
        "    print(f\"\\nâœ… Successfully loaded {len(df)} benchmark results\")\n",
        "    print(f\"ðŸ“Š Models: {list(df['model'].unique())}\")\n",
        "    print(f\"ðŸ“Š Concurrency levels: {sorted(df['concurrency'].unique())}\")\n",
        "    print(f\"ðŸŽ¯ Token configs: {list(df['token_config'].unique())}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "df = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display data summary\n",
        "if not df.empty:\n",
        "    print(\"ðŸ“ˆ PERFORMANCE SUMMARY BY CONCURRENCY\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    summary = df.groupby('concurrency').agg({\n",
        "        'rps': 'mean',\n",
        "        'ttft_ms': 'mean', \n",
        "        'itl_ms': 'mean',\n",
        "        'tokens_per_sec': 'mean'\n",
        "    }).round(1)\n",
        "    \n",
        "    for concurrency, row in summary.iterrows():\n",
        "        print(f\"Concurrency {concurrency:2d}: {row['rps']:5.1f} RPS | {row['ttft_ms']:6.1f}ms TTFT | {row['itl_ms']:5.1f}ms ITL | {row['tokens_per_sec']:6.1f} tok/s\")\n",
        "    \n",
        "    print(f\"\\nðŸ’¡ Key Insights:\")\n",
        "    best_rps = df.loc[df['rps'].idxmax()]\n",
        "    best_ttft = df.loc[df['ttft_ms'].idxmin()]\n",
        "    best_itl = df.loc[df['itl_ms'].idxmin()]\n",
        "    \n",
        "    print(f\"   â€¢ Best RPS: {best_rps['rps']:.1f} at concurrency {best_rps['concurrency']} ({best_rps['token_config']})\")\n",
        "    print(f\"   â€¢ Best TTFT: {best_ttft['ttft_ms']:.1f}ms at concurrency {best_ttft['concurrency']} ({best_ttft['token_config']})\")\n",
        "    print(f\"   â€¢ Best ITL: {best_itl['itl_ms']:.1f}ms at concurrency {best_itl['concurrency']} ({best_itl['token_config']})\")\n",
        "else:\n",
        "    print(\"âŒ No data to analyze\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple performance charts\n",
        "if not df.empty:\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Concurrency vs RPS', 'Concurrency vs TTFT', \n",
        "                       'Concurrency vs ITL', 'RPS vs Latency Trade-offs')\n",
        "    )\n",
        "    \n",
        "    colors = ['red', 'blue', 'green', 'orange']\n",
        "    \n",
        "    for i, token_config in enumerate(df['token_config'].unique()):\n",
        "        data = df[df['token_config'] == token_config].sort_values('concurrency')\n",
        "        color = colors[i % len(colors)]\n",
        "        \n",
        "        # 1. Concurrency vs RPS\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=data['concurrency'], y=data['rps'], \n",
        "                      mode='lines+markers', name=f'{token_config} RPS',\n",
        "                      line=dict(color=color, width=3), marker=dict(size=8)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. Concurrency vs TTFT\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=data['concurrency'], y=data['ttft_ms'], \n",
        "                      mode='lines+markers', name=f'{token_config} TTFT',\n",
        "                      line=dict(color=color, dash='dash'), showlegend=False),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Concurrency vs ITL\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=data['concurrency'], y=data['itl_ms'], \n",
        "                      mode='lines+markers', name=f'{token_config} ITL',\n",
        "                      line=dict(color=color, dash='dot'), showlegend=False),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. RPS vs TTFT with ITL as size\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=data['ttft_ms'], y=data['rps'], \n",
        "                      mode='markers', name=f'{token_config} Trade-off',\n",
        "                      marker=dict(size=data['itl_ms']/10, color=color),\n",
        "                      text=data['concurrency'], showlegend=False),\n",
        "            row=2, col=2\n",
        "        )\n",
        "    \n",
        "    fig.update_layout(height=800, title_text=\"ðŸ“Š Performance Analysis Dashboard\")\n",
        "    fig.update_xaxes(title_text=\"Concurrency\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"RPS\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Concurrency\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"TTFT (ms)\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Concurrency\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"ITL (ms)\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"TTFT (ms)\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"RPS\", row=2, col=2)\n",
        "    \n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"âŒ No data to plot\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Working interactive calculator with model support\n",
        "def gpu_calculator(target_rps=10, target_concurrency=25, model='All', token_config='All'):\n",
        "    \"\"\"GPU requirements calculator with model selection\"\"\"\n",
        "    \n",
        "    if df.empty:\n",
        "        print(\"âŒ No data available\")\n",
        "        return\n",
        "    \n",
        "    # Filter data by model and token config\n",
        "    data = df.copy()\n",
        "    \n",
        "    if model != 'All':\n",
        "        data = data[data['model'] == model]\n",
        "    \n",
        "    if token_config != 'All':\n",
        "        data = data[data['token_config'] == token_config]\n",
        "    \n",
        "    if data.empty:\n",
        "        print(f\"âŒ No data for model: {model}, token config: {token_config}\")\n",
        "        return\n",
        "    \n",
        "    # Find closest match\n",
        "    data = data.copy()\n",
        "    data['concurrency_diff'] = abs(data['concurrency'] - target_concurrency)\n",
        "    closest = data.loc[data['concurrency_diff'].idxmin()]\n",
        "    \n",
        "    print(f\"ðŸŽ¯ ANALYSIS FOR {target_rps} RPS, CONCURRENCY {target_concurrency}\")\n",
        "    print(f\"Model: {model} | Token config: {token_config}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    print(f\"ðŸ“Š CLOSEST BENCHMARK:\")\n",
        "    print(f\"   â€¢ Model: {closest['model']}\")\n",
        "    print(f\"   â€¢ Profile: {closest['profile']}\")\n",
        "    print(f\"   â€¢ Concurrency: {closest['concurrency']}\")\n",
        "    print(f\"   â€¢ Actual RPS: {closest['rps']:.1f}\")\n",
        "    print(f\"   â€¢ TTFT: {closest['ttft_ms']:.1f} ms\")\n",
        "    print(f\"   â€¢ ITL: {closest['itl_ms']:.1f} ms\")\n",
        "    print(f\"   â€¢ GPUs: {closest['gpus_used']}\")\n",
        "    \n",
        "    # Calculate scaling\n",
        "    if closest['rps'] > 0:\n",
        "        scale_factor = target_rps / closest['rps']\n",
        "        needed_gpus = max(1, int(np.ceil(closest['gpus_used'] * scale_factor)))\n",
        "        needed_servers = int(np.ceil(needed_gpus / 8))\n",
        "        \n",
        "        # Estimates\n",
        "        est_rps = closest['rps'] * (needed_gpus / closest['gpus_used'])\n",
        "        est_ttft = closest['ttft_ms']  # Constant\n",
        "        est_itl = closest['itl_ms']   # Constant\n",
        "        \n",
        "        print(f\"\\\\nðŸš€ SCALING TO {target_rps} RPS:\")\n",
        "        print(f\"   â€¢ Required GPUs: {needed_gpus}\")\n",
        "        print(f\"   â€¢ Required Servers: {needed_servers}\")\n",
        "        print(f\"   â€¢ Estimated RPS: {est_rps:.1f}\")\n",
        "        print(f\"   â€¢ Expected TTFT: {est_ttft:.1f} ms\")\n",
        "        print(f\"   â€¢ Expected ITL: {est_itl:.1f} ms\")\n",
        "        \n",
        "        # Cost\n",
        "        monthly_cost = (needed_gpus * 375 + needed_servers * 1250)  # Simplified costs\n",
        "        print(f\"   â€¢ Est. Monthly Cost: ${monthly_cost:,.0f}\")\n",
        "        \n",
        "        # User experience assessment\n",
        "        print(f\"\\\\nðŸ‘¤ USER EXPERIENCE:\")\n",
        "        if est_ttft < 200:\n",
        "            print(f\"   âœ… TTFT: Good ({est_ttft:.1f}ms)\")\n",
        "        else:\n",
        "            print(f\"   âš ï¸ TTFT: Slow ({est_ttft:.1f}ms)\")\n",
        "            \n",
        "        if est_itl < 50:\n",
        "            print(f\"   âœ… ITL: Smooth streaming ({est_itl:.1f}ms)\")\n",
        "        elif est_itl < 100:\n",
        "            print(f\"   âš ï¸ ITL: Acceptable streaming ({est_itl:.1f}ms)\")\n",
        "        else:\n",
        "            print(f\"   âŒ ITL: Choppy streaming ({est_itl:.1f}ms)\")\n",
        "\n",
        "# Create the interactive widget with model selection\n",
        "if not df.empty:\n",
        "    model_options = ['All'] + list(df['model'].unique())\n",
        "    token_options = ['All'] + list(df['token_config'].unique())\n",
        "    \n",
        "    print(\"ðŸŽ® INTERACTIVE GPU REQUIREMENTS CALCULATOR\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸ’¡ Includes ITL analysis for streaming quality!\")\n",
        "    print()\n",
        "    \n",
        "    interact(gpu_calculator,\n",
        "             target_rps=IntSlider(value=10, min=1, max=200, step=1, description='Target RPS:'),\n",
        "             target_concurrency=IntSlider(value=25, min=1, max=50, step=1, description='Concurrency:'),\n",
        "             model=Dropdown(options=model_options, value='All', description='Model:'),\n",
        "             token_config=Dropdown(options=token_options, value='All', description='Token Config:'))\n",
        "else:\n",
        "    print(\"âŒ Cannot create calculator - no data loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick reference table\n",
        "if not df.empty:\n",
        "    print(\"ðŸ“‹ QUICK REFERENCE TABLE\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create a simple summary table\n",
        "    summary_table = df.pivot_table(\n",
        "        index='concurrency',\n",
        "        columns='token_config', \n",
        "        values=['rps', 'ttft_ms', 'itl_ms'],\n",
        "        aggfunc='mean'\n",
        "    ).round(1)\n",
        "    \n",
        "    display(summary_table)\n",
        "    \n",
        "    print(\"\\nðŸŽ¯ GPU SCALING REFERENCE (Linear Model):\")\n",
        "    print(\"Based on best performing configuration:\")\n",
        "    \n",
        "    best_rps_per_gpu = df['rps_per_gpu'].max()\n",
        "    \n",
        "    scaling_examples = [1, 5, 10, 25, 50, 100]\n",
        "    for target in scaling_examples:\n",
        "        gpus = max(1, int(np.ceil(target / best_rps_per_gpu)))\n",
        "        servers = int(np.ceil(gpus / 8))\n",
        "        print(f\"   â€¢ {target:3d} RPS â†’ {gpus:2d} GPUs ({servers} servers)\")\n",
        "else:\n",
        "    print(\"âŒ No data for reference table\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Understanding the Metrics\n",
        "\n",
        "### ðŸ“Š **Key Differences:**\n",
        "- **Concurrency**: How many requests processed simultaneously\n",
        "- **RPS**: How many requests completed per second  \n",
        "- **TTFT**: Time to first token (user wait time)\n",
        "- **ITL**: Inter-token latency (streaming quality)\n",
        "\n",
        "### ðŸ’¡ **Why ITL Matters:**\n",
        "- **Low ITL (<50ms)**: Smooth, natural streaming like ChatGPT\n",
        "- **Medium ITL (50-100ms)**: Acceptable but noticeable pauses\n",
        "- **High ITL (>100ms)**: Choppy, slow feeling responses\n",
        "\n",
        "### ðŸŽ¯ **Optimization Strategy:**\n",
        "1. **For throughput**: Increase concurrency until RPS peaks\n",
        "2. **For user experience**: Keep TTFT <200ms, ITL <50ms\n",
        "3. **For scaling**: Add GPUs linearly for higher RPS\n",
        "4. **Sweet spot**: Balance concurrency for max RPS without bad ITL\n",
        "\n",
        "---\n",
        "\n",
        "*Use the interactive calculator above to explore different scenarios!*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
